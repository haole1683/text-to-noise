W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.
1
1
1
11

1
1
1
08/15/2023 14:40:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
08/15/2023 14:40:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/share/ckpt/songtianwei/runs/Aug15_14-40-17_fvl14,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=100000,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=15.0,
optim=adamw_hf,
optim_args=None,
output_dir=/share/ckpt/songtianwei,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=128,
per_device_train_batch_size=128,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['wandb'],
resume_from_checkpoint=None,
run_name=/share/ckpt/songtianwei,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.1,
xpu_backend=None,
)
08/15/2023 14:40:19 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl
Num processes: 8
Process index: 0
Local process index: 0
Device: cuda:0

Mixed precision type: no

Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:19 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:19 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:19 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:22 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:22 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:24 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:24 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
Repo card metadata block was not found. Setting CardData to empty.
08/15/2023 14:40:24 - WARNING - huggingface_hub.repocard - Repo card metadata block was not found. Setting CardData to empty.
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:7
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:6
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
08/15/2023 14:53:31 - INFO - __main__ - ***** Running training *****
08/15/2023 14:53:31 - INFO - __main__ -   Training num examples = 591753
08/15/2023 14:53:31 - INFO - __main__ -   Evaluation num examples = 25014
08/15/2023 14:53:31 - INFO - __main__ -   Num Epochs = 15
08/15/2023 14:53:31 - INFO - __main__ -   Instantaneous batch size per device = 128
08/15/2023 14:53:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1024
08/15/2023 14:53:31 - INFO - __main__ -   Gradient Accumulation steps = 1
08/15/2023 14:53:31 - INFO - __main__ -   Total optimization steps = 100000
  0%|          | 0/100000 [00:00<?, ?it/s]Training Steps:   0%|          | 0/100000 [00:00<?, ?it/s]wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:2
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:1
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:5
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
THE DEVICE cuda:4
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
wandb: Tracking run with wandb version 0.15.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
THE DEVICE cuda:3
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - __main__ - Accelerator.device cuda:0
08/15/2023 14:53:31 - INFO - __main__ - GPU_NUM: 8
08/15/2023 14:53:31 - INFO - __main__ - clip_train: True, generator_train: False
08/15/2023 14:53:31 - INFO - __main__ - add_noise: False, use_normailize:True
THE DEVICE cuda:0
08/15/2023 14:53:31 - INFO - root - **************************************************
08/15/2023 14:53:31 - INFO - root - Doing Training
08/15/2023 14:53:31 - INFO - root - **************************************************
Training Steps:   0%|          | 0/100000 [00:00<?, ?it/s][W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Training Steps:   0%|          | 1/100000 [1:09:58<116627:30:16, 4198.63s/it]Training Steps:   0%|          | 1/100000 [1:09:58<116627:30:16, 4198.63s/it, epoch=0, global_step=1, lr=5e-5, step=0, train_loss=4.9]