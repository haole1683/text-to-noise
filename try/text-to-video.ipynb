{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb0070c-52c3-4abf-aa51-55a551a4edbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
    "from diffusers.utils import export_to_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f02e31f-9413-4c17-bc63-036ea991371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pipeline\n",
    "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62782da2-db76-4eaf-a9c8-6fe8781d58b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize for GPU memory\n",
    "pipe.enable_model_cpu_offload()\n",
    "pipe.enable_vae_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e8e2ba7-bf7a-489f-b122-a59fa33de543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb6cf05f8374813aa249481162d9028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"A group of man are celebrating the festival\"\n",
    "video_frames = pipe(prompt, num_inference_steps=25, num_frames=200,height=224,width=224).frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c3f2d62-9def-4bf7-b3a4-773238fa3bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m     \n",
       "\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwidth\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_frames\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_inference_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mguidance_scale\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0meta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgenerator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlatents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_embeds\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'np'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcross_attention_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m           TextToVideoSDPipeline\n",
       "\u001b[0;31mString form:\u001b[0m   \n",
       "TextToVideoSDPipeline {\n",
       "           \"_class_name\": \"TextToVideoSDPipeline\",\n",
       "           \"_diffusers_version\": \"0.17.0 <...> iffusers\",\n",
       "           \"UNet3DConditionModel\"\n",
       "           ],\n",
       "           \"vae\": [\n",
       "           \"diffusers\",\n",
       "           \"AutoencoderKL\"\n",
       "           ]\n",
       "           }\n",
       "           \n",
       "\u001b[0;31mFile:\u001b[0m           ~/conda/envs/pytorch2/lib/python3.8/site-packages/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Pipeline for text-to-video generation.\n",
       "\n",
       "This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods the\n",
       "library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)\n",
       "\n",
       "Args:\n",
       "    vae ([`AutoencoderKL`]):\n",
       "        Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
       "    text_encoder ([`CLIPTextModel`]):\n",
       "        Frozen text-encoder. Same as Stable Diffusion 2.\n",
       "    tokenizer (`CLIPTokenizer`):\n",
       "        Tokenizer of class\n",
       "        [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
       "    unet ([`UNet3DConditionModel`]): Conditional U-Net architecture to denoise the encoded video latents.\n",
       "    scheduler ([`SchedulerMixin`]):\n",
       "        A scheduler to be used in combination with `unet` to denoise the encoded image latents. Can be one of\n",
       "        [`DDIMScheduler`], [`LMSDiscreteScheduler`], or [`PNDMScheduler`].\n",
       "\u001b[0;31mCall docstring:\u001b[0m\n",
       "    Function invoked when calling the pipeline for generation.\n",
       "\n",
       "    Args:\n",
       "        prompt (`str` or `List[str]`, *optional*):\n",
       "            The prompt or prompts to guide the video generation. If not defined, one has to pass `prompt_embeds`.\n",
       "            instead.\n",
       "        height (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
       "            The height in pixels of the generated video.\n",
       "        width (`int`, *optional*, defaults to self.unet.config.sample_size * self.vae_scale_factor):\n",
       "            The width in pixels of the generated video.\n",
       "        num_frames (`int`, *optional*, defaults to 16):\n",
       "            The number of video frames that are generated. Defaults to 16 frames which at 8 frames per seconds\n",
       "            amounts to 2 seconds of video.\n",
       "        num_inference_steps (`int`, *optional*, defaults to 50):\n",
       "            The number of denoising steps. More denoising steps usually lead to a higher quality videos at the\n",
       "            expense of slower inference.\n",
       "        guidance_scale (`float`, *optional*, defaults to 7.5):\n",
       "            Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
       "            `guidance_scale` is defined as `w` of equation 2. of [Imagen\n",
       "            Paper](https://arxiv.org/pdf/2205.11487.pdf). Guidance scale is enabled by setting `guidance_scale >\n",
       "            1`. Higher guidance scale encourages to generate videos that are closely linked to the text `prompt`,\n",
       "            usually at the expense of lower video quality.\n",
       "        negative_prompt (`str` or `List[str]`, *optional*):\n",
       "            The prompt or prompts not to guide the video generation. If not defined, one has to pass\n",
       "            `negative_prompt_embeds` instead. Ignored when not using guidance (i.e., ignored if `guidance_scale` is\n",
       "            less than `1`).\n",
       "        eta (`float`, *optional*, defaults to 0.0):\n",
       "            Corresponds to parameter eta (Î·) in the DDIM paper: https://arxiv.org/abs/2010.02502. Only applies to\n",
       "            [`schedulers.DDIMScheduler`], will be ignored for others.\n",
       "        generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
       "            One or a list of [torch generator(s)](https://pytorch.org/docs/stable/generated/torch.Generator.html)\n",
       "            to make generation deterministic.\n",
       "        latents (`torch.FloatTensor`, *optional*):\n",
       "            Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for video\n",
       "            generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
       "            tensor will ge generated by sampling using the supplied random `generator`. Latents should be of shape\n",
       "            `(batch_size, num_channel, num_frames, height, width)`.\n",
       "        prompt_embeds (`torch.FloatTensor`, *optional*):\n",
       "            Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
       "            provided, text embeddings will be generated from `prompt` input argument.\n",
       "        negative_prompt_embeds (`torch.FloatTensor`, *optional*):\n",
       "            Pre-generated negative text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt\n",
       "            weighting. If not provided, negative_prompt_embeds will be generated from `negative_prompt` input\n",
       "            argument.\n",
       "        output_type (`str`, *optional*, defaults to `\"np\"`):\n",
       "            The output format of the generate video. Choose between `torch.FloatTensor` or `np.array`.\n",
       "        return_dict (`bool`, *optional*, defaults to `True`):\n",
       "            Whether or not to return a [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] instead of a\n",
       "            plain tuple.\n",
       "        callback (`Callable`, *optional*):\n",
       "            A function that will be called every `callback_steps` steps during inference. The function will be\n",
       "            called with the following arguments: `callback(step: int, timestep: int, latents: torch.FloatTensor)`.\n",
       "        callback_steps (`int`, *optional*, defaults to 1):\n",
       "            The frequency at which the `callback` function will be called. If not specified, the callback will be\n",
       "            called at every step.\n",
       "        cross_attention_kwargs (`dict`, *optional*):\n",
       "            A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under\n",
       "            `self.processor` in\n",
       "            [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).\n",
       "\n",
       "\n",
       "Examples:\n",
       "    ```py\n",
       "    >>> import torch\n",
       "    >>> from diffusers import TextToVideoSDPipeline\n",
       "    >>> from diffusers.utils import export_to_video\n",
       "\n",
       "    >>> pipe = TextToVideoSDPipeline.from_pretrained(\n",
       "    ...     \"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\"\n",
       "    ... )\n",
       "    >>> pipe.enable_model_cpu_offload()\n",
       "\n",
       "    >>> prompt = \"Spiderman is surfing\"\n",
       "    >>> video_frames = pipe(prompt).frames\n",
       "    >>> video_path = export_to_video(video_frames)\n",
       "    >>> video_path\n",
       "    ```\n",
       "\n",
       "\n",
       "    Returns:\n",
       "        [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] or `tuple`:\n",
       "        [`~pipelines.stable_diffusion.TextToVideoSDPipelineOutput`] if `return_dict` is True, otherwise a `tuple.\n",
       "        When returning a tuple, the first element is a list with the generated frames.\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8fbc07c-27d5-4af6-a1c3-6f62188a0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = export_to_video(video_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd16882c-d992-495f-9b9c-3f45bbb1c79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpm8cdtm76.mp4\n"
     ]
    }
   ],
   "source": [
    "print(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31ce72dd-b71b-4b8e-9607-529c20268a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /tmp/tmpm8cdtm76.mp4 .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e863fe9e-3680-445a-9770-40aa281d14a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"tmpeadhfdnp.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "\n",
    "display(Video(\"tmpeadhfdnp.mp4\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a32b0-ef66-4a11-9ac6-f7a89aad33ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
