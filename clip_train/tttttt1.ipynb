{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b309fd4-fd9f-423e-a4be-1667334e6dcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23d9ffa-58fe-4811-a1d3-513618dfe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, ToTensor\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry,ContextManagers\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.utils import is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2435a-90f6-4f9c-9e90-7021e3882656",
   "metadata": {},
   "source": [
    "## Initialize sh Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a1a5b-d34d-4d20-a5f1-c1b6a8931101",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c212fe7-7baa-4426-987f-0c5b7ae7cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '--output_dir', './clip-roberta-finetuned',\n",
    "    '--model_name_or_path', '/remote-home/songtianwei/research/diffusion_model_my/clip-roberta',\n",
    "    '--data_dir', '/remote-home/songtianwei/research/diffusion_model_my/data',\n",
    "    '--dataset_name', 'ydshieh/coco_dataset_script',\n",
    "    '--dataset_config_name', '2017',\n",
    "    '--image_column', 'image_path',\n",
    "    '--caption_column', 'caption',\n",
    "    '--remove_unused_columns', 'False',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    '--report_to',\"wandb\" ,\n",
    "    '--learning_rate',\"5e-5\",\n",
    "    '--warmup_steps',\"0\" ,\n",
    "    '--weight_decay',\"0.1\" ,\n",
    "    '--overwrite_output_dir' ,\n",
    "    '--max_seq_length',\"77\" ,\n",
    "    '--max_steps',\"30000\" ,\n",
    "    '--per_device_train_batch_size',\"6\" ,\n",
    "    '--per_device_eval_batch_size',\"6\" ,\n",
    "    '--max_train_samples',\"5000\",\n",
    "    '--max_eval_samples',\"1000\" ,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c6ec2-53c3-4fdc-8f99-81ab5551b39d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initilize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca4e3dbd-9d8b-408c-8864-fa9ecbd6075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.32.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "234cfd0d-d8ee-42f4-aa4c-0558d2598fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_vision_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n",
    "    )\n",
    "    freeze_text_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad35fec3-80cc-477c-b149-ccd1629b6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n",
    "    image_column: Optional[str] = field(\n",
    "        default=\"image_path\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n",
    "    )\n",
    "    caption_column: Optional[str] = field(\n",
    "        default=\"caption\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input testing data file (a jsonlines file).\"},\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    \n",
    "    # Noise type, default is none, other noise is \"random\" and \"clip_min_noise\"\n",
    "    dataset_noise_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The type of noise to add to the dataset.\"},\n",
    "    )\n",
    "    \n",
    "    dataset_normalize_flag: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to normalize the dataset.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension == \"json\", \"`validation_file` should be a json file.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd1219-f42b-431b-ba95-b167f59f9266",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dataset name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efb0625d-a9bd-47b8-b81d-99a6de35ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92294afc-e160-42e8-82e4-f4bf16d77ebe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c277af71-6822-44c0-84d4-149d80d30ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean=None, std=None):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms.Compose([\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC,antialias=None),\n",
    "            CenterCrop(image_size),  # CenterCrop is required because Resize doesn't ensure same output size\n",
    "            # ConvertImageDtype(torch.float),\n",
    "            ToTensor(), \n",
    "        ])\n",
    "        if mean is not None and std is not None:\n",
    "            self.transforms.transforms.append(Normalize(mean=mean, std=std))\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96934479-c706-4f46-8509-097f25616079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_fn(x, mean, std):\n",
    "    return Normalize(mean=mean, std=std)(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01437c-3218-400d-a051-e8b769339776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c19475d-beff-454a-bf1d-8221305e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3660b34-6389-4b7f-b969-acea9fac7cf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6098a8fb-551e-4820-afda-6ff1037de887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.unet_config = {\n",
    "            \"act_fn\": \"silu\",\n",
    "            \"attention_head_dim\": 8,\n",
    "            \"block_out_channels\": [\n",
    "                320,\n",
    "                640,\n",
    "                1280,\n",
    "                1280\n",
    "            ],\n",
    "            \"center_input_sample\": False,\n",
    "            \"cross_attention_dim\": 768,  # NOTE 768\n",
    "            \"down_block_types\": [\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"DownBlock2D\"\n",
    "            ],\n",
    "            \"downsample_padding\": 1,\n",
    "            \"flip_sin_to_cos\": True,\n",
    "            \"freq_shift\": 0,\n",
    "            \"in_channels\": 4,\n",
    "            \"layers_per_block\": 2,\n",
    "            \"mid_block_scale_factor\": 1,\n",
    "            \"norm_eps\": 1e-05,\n",
    "            \"norm_num_groups\": 32,\n",
    "            \"out_channels\": 4,\n",
    "            \"sample_size\": 224,\n",
    "            \"up_block_types\": [\n",
    "                \"UpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\"\n",
    "            ]\n",
    "        }\n",
    "        self.unet = UNet2DConditionModel(**self.unet_config)\n",
    "        self.vae_config = {\n",
    "            'in_channels': 3,\n",
    "            'out_channels': 3,\n",
    "            'down_block_types': ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "            'up_block_types': ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "            'block_out_channels': [128, 256, 512, 512],\n",
    "            'layers_per_block': 2,\n",
    "            'act_fn': 'silu',\n",
    "            'latent_channels': 4,\n",
    "            'norm_num_groups': 32,\n",
    "            'sample_size': 512,\n",
    "            'scaling_factor': 0.18215,\n",
    "        }\n",
    "        self.vae = AutoencoderKL(**self.vae_config)\n",
    "        \n",
    "    def forward(self, img_pixel_values, encoder_hidden_states):\n",
    "        latent = self.vae.encode(img_pixel_values).latent_dist.sample()\n",
    "        timesteps = torch.randint(0, 1000, (1,),device=latent.device)\n",
    "        timesteps = timesteps.long()  #  6\n",
    "        unet_pred = self.unet(latent, timesteps, encoder_hidden_states).sample\n",
    "        vae_decoding = self.vae.decoder(unet_pred)\n",
    "        return vae_decoding\n",
    "    \n",
    "    def enable_xformers_memory_efficient_attention(self):\n",
    "        self.unet.enable_xformers_memory_efficient_attention()\n",
    "        self.vae.enable_xformers_memory_efficient_attention()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dcadf-092f-430e-b820-da0d3ce7f122",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Parse input arguments\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c8bbf5-3651-4ef7-8350-2841dcda181d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    # If we pass only one argument to the script and it's the path to a json file,\n",
    "    # let's parse it to get our arguments.\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "else:\n",
    "    print(\"1\")\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=args_list)\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_clip\", model_args, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a88a85-c717-48dd-92e7-4e234d225fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wandb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.report_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf2b34-aeca-4188-bbfc-16d171a41bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe3859d-a1a9-4d83-927c-ce1f449f5e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "08/03/2023 21:25:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./clip-roberta-finetuned/runs/Aug03_21-25-06_00351cb6f98e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=30000,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./clip-roberta-finetuned,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=6,\n",
      "per_device_train_batch_size=6,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./clip-roberta-finetuned,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56937d-9b0c-426e-920f-d1c0e027c0b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.Initialize accelerator and distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5b6838-2181-4b07-8db3-27a70c0d7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:06 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(total_limit=training_args.save_total_limit)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "    mixed_precision=\"no\",\n",
    "    log_with=training_args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f4cc4-3ee8-4562-8a13-9c1c8e12d67a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 4. Detecting last checkpoint and eventualy continue from last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41e623e-f8fb-4652-b6fa-69f8520ac144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb3f1a-672d-41b8-a708-a8adf63b4274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f02d884-9bf8-4ee4-91a5-e8a49800989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:12 - WARNING - datasets.builder - Found cached dataset coco_dataset_script (/remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ba6cc3af1e48e8bb17316971b6756f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    dataset = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        keep_in_memory=False,\n",
    "        data_dir=data_args.data_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.validation_file.split(\".\")[-1]\n",
    "    if data_args.test_file is not None:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "        extension = data_args.test_file.split(\".\")[-1]\n",
    "    dataset = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be6dec-ba9a-4ea1-8150-616428eb9c0f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Load pretrained model, tokenizer, and image processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8534f123-a3ee-4f45-8874-ac19389c9f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64ef6fb0-db38-4151-b2fa-f09fcc446a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09695a11-6475-496e-a6a0-8fcf63631a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/remote-home/songtianwei/research/diffusion_model_my/clip-roberta'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c985349-d79e-4744-9089-a1ae28283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb8166e-f412-4219-8564-e3dae58d3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Do not use the clip tokenizer, loss can not decrease\n",
    "use_clip_tokenizer = False\n",
    "if use_clip_tokenizer:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=revision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2454ecd5-1947-4366-a89b-afa97900590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load image_processor, in this script we only use this to get the mean and std for normalization.\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_args.image_processor_name or model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c633b2-1155-46b9-bc9d-d15a953d12a8",
   "metadata": {},
   "source": [
    "### clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b398b6c4-3326-4f13-a349-80d43f7b1d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clip_model = AutoModel.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "clip_model_config = clip_model.config\n",
    "clip_pretrained = False\n",
    "if clip_pretrained:\n",
    "    pass\n",
    "else:\n",
    "    clip_model = AutoModel.from_config(clip_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d00ff33-6f7a-4032-aad8-ad1d81788cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _freeze_params(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if model_args.freeze_vision_model:\n",
    "    _freeze_params(clip_model.vision_model)\n",
    "\n",
    "if model_args.freeze_text_model:\n",
    "    _freeze_params(clip_model.text_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0612282-584f-4195-ac38-09353337d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.seed is not None:\n",
    "    set_seed(training_args.seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906cd47-7032-4e1d-88f9-a2da85b8ebf3",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e21e44e8-dc3f-44eb-92c8-50d6dc03434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b2ffe33-1155-40dd-be89-62ef795bb28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger.info(f\"generator_train: {generator_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75fbd4-8454-4d63-8fba-aed91d053941",
   "metadata": {},
   "source": [
    "### text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed329fa-7106-4a84-884c-d5c95db753ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=revision\n",
    ")\n",
    "# text_encoder\n",
    "text_encoder.requires_grad_(False)\n",
    "weight_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3af7-bf46-4674-aa59-0caec2234a92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Get the column names for input/target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "039dca28-c8f8-4157-a3d9-a66d68e2769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "if training_args.do_train:\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = dataset[\"validation\"].column_names\n",
    "elif training_args.do_predict:\n",
    "    column_names = dataset[\"test\"].column_names\n",
    "else:\n",
    "    logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43b81f2e-7dbf-4b97-8195-d062507cb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n",
    "if data_args.image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = data_args.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if data_args.caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = data_args.caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c6f5b-1a5c-41a0-9485-63762318eae5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46f47720-5543-44fc-a6e1-69cd4c1827ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_path'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9288390-f59e-49d2-baf3-610760b260e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_transformations = Transform(\n",
    "    clip_model_config.vision_config.image_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcc987-1453-4e7f-98db-57a1777676f7",
   "metadata": {},
   "source": [
    "### tokenize_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6b63236-4f9e-4379-9359-e35363e5ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(examples):\n",
    "    captions = list(examples[caption_column])\n",
    "    text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be3e40-692b-4442-bed2-3231fede4f77",
   "metadata": {},
   "source": [
    "### transform_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d63887b-6cb6-426f-a2dd-0b25f14f8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_images(examples):\n",
    "    if isinstance(examples[image_column][0],str):\n",
    "        # For coco dataset, the images are loaded as path\n",
    "        # images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        images = [Image.open(image_file).convert(\"RGB\") for image_file in examples[image_column]]\n",
    "    else:\n",
    "        # lambdalabs/pokemon-blip-captions\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b852ada-5ba8-4dde-940a-ad4cfbe1c9ad",
   "metadata": {},
   "source": [
    "### filter_corrupt_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2a3adef-58c4-42c2-b936-a4d8aa724c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_corrupt_images(examples):\n",
    "    \"\"\"remove problematic images\"\"\"\n",
    "    valid_images = []\n",
    "    for image_file in examples[image_column]:\n",
    "        try:\n",
    "            Image.open(image_file).convert(\"RGB\") \n",
    "            valid_images.append(True)\n",
    "        except Exception:\n",
    "            valid_images.append(False)\n",
    "    return valid_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266c149-5dcb-4999-a909-e2bdb2b18e5b",
   "metadata": {},
   "source": [
    "### do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f3ccb99-f700-48d1-91c1-ddd9b28e8fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-82b6cdfcbcb917f2.arrow\n",
      "08/03/2023 21:25:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-0be60cf72203d467.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_train:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"train\" not in dataset:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "        # print(len(train_dataset))\n",
    "        train_dataset = train_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        \n",
    "        train_dataset = train_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        train_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65103fe4-cb79-4144-ba80-3819f9cbe31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in train_dataset:\n",
    "#     print(data)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30f5e5e4-49fa-4785-a63b-81c1a7d0739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_path', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4097\n",
       "})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "892c3967-397d-457a-abb0-4d316a65188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,  # here change to False to check the order of the images\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=training_args.train_batch_size,\n",
    "        num_workers=training_args.dataloader_num_workers,\n",
    "        drop_last=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "062a52ae-7c8f-4a6c-9db3-902e6daf6953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx,batch in enumerate(train_dataloader):\n",
    "#     if idx>4:\n",
    "#         print(batch)\n",
    "#     if(idx>7):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceee9c-5683-4a33-b387-5d5c369d34b7",
   "metadata": {},
   "source": [
    "### do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fec716e7-cc64-49e2-a274-51c61ffe6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-cf9af83defcb0423.arrow\n",
      "08/03/2023 21:25:39 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-0717f5122bad88ef.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_eval:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"validation\" not in dataset:\n",
    "            raise ValueError(\"--do_eval requires a train validation\")\n",
    "        eval_dataset = dataset[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        eval_dataset.set_transform(transform_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0de79590-485c-4880-984a-c131cb1093d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation dataloader\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=training_args.eval_batch_size,\n",
    "    num_workers=training_args.dataloader_num_workers,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71221094-a8f5-42ba-a4fc-7b30e8159769",
   "metadata": {},
   "source": [
    "### do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a79ce7d-26e7-4d07-b80a-e87f010893be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.do_predict:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"test\" not in dataset:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)\n",
    "            test_dataset = test_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        test_dataset = test_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        test_dataset = test_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        test_dataset.set_transform(transform_images)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c12de7-0e67-4d0c-b31d-4c7da0aa8e85",
   "metadata": {},
   "source": [
    "### Normailze fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "960c175b-86d2-4911-af5a-9a5014bf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fn(x, mean, std):\n",
    "    return transforms.Normalize(mean=mean, std=std)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3e7b3-290b-4922-bd5d-a57ca42653da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9.Initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25082c8f-a4b6-49ba-8fee-95bdd96b935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/songtianwei/conda/envs/pytorch2/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/remote-home/songtianwei/conda/envs/pytorch2/lib/libcudart.so.11.0'), PosixPath('/remote-home/songtianwei/conda/envs/pytorch2/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /remote-home/songtianwei/conda/envs/pytorch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /remote-home/songtianwei/conda/envs/pytorch2/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /remote-home/songtianwei/conda/envs/pytorch2/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    }
   ],
   "source": [
    "use_8bit_adam = True\n",
    "if use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "        )\n",
    "\n",
    "    optimizer_cls = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_cls = torch.optim.AdamW\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8358e129-4555-4631-bc97-963082540582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitsandbytes.optim.adamw.AdamW8bit"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63efdce7-4e75-4094-9aac-ec9596ba3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(clip_model, ALL_LAYERNORM_LAYERS)\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.1,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "adam_kwargs = {\n",
    "    \"lr\": 5e-5,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-8,\n",
    "}\n",
    "\n",
    "optimizer = optimizer_cls(optimizer_grouped_parameters, **adam_kwargs)\n",
    "optimizer_generator = optimizer_cls(generator.parameters(), **adam_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1472ae2-0853-474c-a0a6-59384da85567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vision_model.vision_model.embeddings.patch_embedding.weight',\n",
       " 'vision_model.vision_model.embeddings.position_embedding.weight',\n",
       " 'vision_model.vision_model.embeddings.class_embedding',\n",
       " 'vision_model.vision_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'vision_model.vision_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'text_model.embeddings.word_embeddings.weight',\n",
       " 'text_model.embeddings.position_embeddings.weight',\n",
       " 'text_model.embeddings.token_type_embeddings.weight',\n",
       " 'text_model.encoder.layer.0.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.0.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.0.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.0.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.0.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.0.output.dense.weight',\n",
       " 'text_model.encoder.layer.1.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.1.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.1.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.1.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.1.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.1.output.dense.weight',\n",
       " 'text_model.encoder.layer.2.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.2.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.2.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.2.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.2.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.2.output.dense.weight',\n",
       " 'text_model.encoder.layer.3.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.3.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.3.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.3.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.3.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.3.output.dense.weight',\n",
       " 'text_model.encoder.layer.4.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.4.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.4.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.4.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.4.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.4.output.dense.weight',\n",
       " 'text_model.encoder.layer.5.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.5.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.5.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.5.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.5.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.5.output.dense.weight',\n",
       " 'text_model.encoder.layer.6.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.6.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.6.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.6.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.6.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.6.output.dense.weight',\n",
       " 'text_model.encoder.layer.7.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.7.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.7.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.7.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.7.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.7.output.dense.weight',\n",
       " 'text_model.encoder.layer.8.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.8.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.8.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.8.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.8.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.8.output.dense.weight',\n",
       " 'text_model.encoder.layer.9.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.9.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.9.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.9.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.9.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.9.output.dense.weight',\n",
       " 'text_model.encoder.layer.10.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.10.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.10.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.10.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.10.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.10.output.dense.weight',\n",
       " 'text_model.encoder.layer.11.attention.self.query.weight',\n",
       " 'text_model.encoder.layer.11.attention.self.key.weight',\n",
       " 'text_model.encoder.layer.11.attention.self.value.weight',\n",
       " 'text_model.encoder.layer.11.attention.output.dense.weight',\n",
       " 'text_model.encoder.layer.11.intermediate.dense.weight',\n",
       " 'text_model.encoder.layer.11.output.dense.weight',\n",
       " 'text_model.pooler.dense.weight',\n",
       " 'visual_projection.weight',\n",
       " 'text_projection.weight',\n",
       " 'logit_scale']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf56cf06-957f-4ebc-9e18-1db5b19c0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = 'linear'\n",
    "lr_warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f0cf978-d9c5-4984-99f9-023fb0420263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_scheduler = get_scheduler(\n",
    "#         lr_scheduler,\n",
    "#         optimizer=optimizer,\n",
    "#         num_warmup_steps=lr_warmup_steps * training_args.gradient_accumulation_steps,\n",
    "#         num_training_steps=training_args.max_steps * training_args.gradient_accumulation_steps,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e112dc4f-0960-4793-8822-246050ba7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo Optimizer for generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3b172-b841-4a06-8068-110c50e3d147",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10.Initial About the accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f0cdfd78-0bd1-45b0-9d3d-cc19a8e58513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if training_args.output_dir is not None:\n",
    "        os.makedirs(training_args.output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfb4d7f3-7455-411e-b78e-4532a34f1686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For optimizer and scheduler\n",
    "optimizer = accelerator.prepare(optimizer)\n",
    "# lr_scheduler = accelerator.prepare(lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77112bfd-b1d4-4daf-8e6e-8334386dc011",
   "metadata": {},
   "source": [
    "### load model and optimizer and dataloader to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd8c8ae4-634c-44a8-a3fe-88ea1fc01a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For model\n",
    "add_noise = True\n",
    "clip_model = accelerator.prepare(clip_model)\n",
    "if add_noise:\n",
    "    generator = accelerator.prepare(generator)\n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34bad-acca-4227-8d72-fd702340ce15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11.Initialize max_train_step and tracker ( wandb start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e75bed4-71a1-4392-8f5f-153f464a91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / training_args.gradient_accumulation_steps)\n",
    "if training_args.max_steps is None or training_args.max_steps <= 0:\n",
    "    training_args.max_steps = training_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "training_args.max_steps = (int)(training_args.max_steps)\n",
    "training_args.num_train_epochs = (int)(training_args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dca957db-1862-4ec5-96bc-caed0b3f4fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18a367ca-70e1-41f4-a813-c6dbf7bac805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fe3c937-4085-4d32-8693-9139c025413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_project_name = \"text2image-fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3258147f-8ba0-440b-8239-171a71215133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # We need to initialize the trackers we use, and also store our configuration.\n",
    "# # The trackers initializes automatically on the main process.\n",
    "# if accelerator.is_main_process:\n",
    "#     tracker_config = dict(vars(args))\n",
    "#     # tracker_config.pop(\"validation_prompts\")\n",
    "#     accelerator.init_trackers(tracker_project_name, tracker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed935705-fb7f-4778-9dc9-29f0b286da4b",
   "metadata": {},
   "source": [
    "## 12. Start Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff753a-1391-45e4-a4ac-73813b3ddac7",
   "metadata": {},
   "source": [
    "### log training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4122351f-fd79-4114-ab77-a6fb0925b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:25:44 - INFO - __main__ - ***** Running training *****\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Training num examples = 4097\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Evaluation num examples = 1000\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Num Epochs = 3\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Instantaneous batch size per device = 6\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "08/03/2023 21:25:44 - INFO - __main__ -   Total optimization steps = 30000\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = training_args.train_batch_size * accelerator.num_processes * training_args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "if training_args.do_train:\n",
    "    logger.info(f\"  Training num examples = {len(train_dataset)}\")\n",
    "if training_args.do_eval:\n",
    "    logger.info(f\"  Evaluation num examples = {len(eval_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {training_args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {training_args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {training_args.max_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8fc25-ec5f-4a3e-8b74-dbc307d98096",
   "metadata": {},
   "source": [
    "### resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4d487bd3-0d87-4ac2-b285-a1a2256ba0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.resume_from_checkpoint:\n",
    "    if training_args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(training_args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(training_args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{training_args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        training_args.resume_from_checkpoint = None\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(training_args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        resume_global_step = global_step * training_args.gradient_accumulation_steps\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "        resume_step = resume_global_step % (num_update_steps_per_epoch * training_args.gradient_accumulation_steps)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "66e4fd10-d54c-474b-8926-589b34a19dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|                                                                                                             | 0/30000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(global_step, training_args.max_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02cb61ef-66bb-4f40-91da-336f15a87fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78a164-b92e-41d1-9501-e304e19b9a26",
   "metadata": {},
   "source": [
    "### Training Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a2f36495-eeae-4e9d-b0ec-36bb5fe0c8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:30:36 - INFO - __main__ - clip_train: True, generator_train: False\n",
      "08/03/2023 21:30:36 - INFO - __main__ - add_noise: False, use_normailize: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "add_noise = False\n",
    "\n",
    "generator_train = False\n",
    "clip_train = True\n",
    "\n",
    "if add_noise:\n",
    "    if generator_train:\n",
    "        generator.train()\n",
    "        generator.requires_grad_(True)\n",
    "        generator.zero_grad()\n",
    "    else:\n",
    "        generator.eval()\n",
    "        generator.requires_grad_(False)\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "if clip_train:\n",
    "    clip_model.train()\n",
    "    clip_model.requires_grad_(True)\n",
    "else:\n",
    "    clip_model.eval()\n",
    "    clip_model.requires_grad_(False)\n",
    "clip_model.zero_grad()\n",
    "    \n",
    "use_normailize = True\n",
    "\n",
    "logger.info(\"clip_train: {}, generator_train: {}\".format(clip_train, generator_train))\n",
    "logger.info(f\"add_noise: {add_noise}, use_normailize: {use_normailize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9edbd731-ae46-4eae-907f-ddaebae4feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, batch in enumerate(train_dataloader):\n",
    "#     text_encoder = clip_model.text_model\n",
    "#     batch_pixel_values = batch[\"pixel_values\"]  # [6,3,224,224]\n",
    "#     batch_input_ids = batch[\"input_ids\"]\n",
    "#     batch_attention_mask = batch[\"attention_mask\"]\n",
    "#     print(batch_input_ids.shape)  # [64,128]\n",
    "#     with torch.no_grad():\n",
    "#         encoder_hidden_states = text_encoder(batch_input_ids,batch_attention_mask)[0]  # [6,128,768]     \n",
    "#     print(encoder_hidden_states.shape)\n",
    "#     noise = generator(batch_pixel_values, encoder_hidden_states)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f830c57-e156-43ee-acad-29addb3a1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "214588f7-f75c-44d9-9a49-4072a69f2e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 21:31:06 - INFO - root - **************************************************\n",
      "08/03/2023 21:31:06 - INFO - root - Doing Training\n",
      "08/03/2023 21:31:06 - INFO - root - **************************************************\n",
      "Training Steps:   1%|                                   | 245/30000 [05:21<4:21:38,  1.90it/s, epoch=0, global_step=245, lr=5e-5, step=24, train_loss=1.79]Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n",
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 246/30000 [05:22<39:22:00,  4.76s/it, epoch=0, global_step=246, lr=5e-5, step=0, train_loss=1.79]Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 247/30000 [05:22<28:54:35,  3.50s/it, epoch=0, global_step=247, lr=5e-5, step=1, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 248/30000 [05:23<21:36:08,  2.61s/it, epoch=0, global_step=248, lr=5e-5, step=2, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 249/30000 [05:23<16:20:51,  1.98s/it, epoch=0, global_step=249, lr=5e-5, step=3, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 250/30000 [05:24<12:59:23,  1.57s/it, epoch=0, global_step=250, lr=5e-5, step=4, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 251/30000 [05:25<10:33:03,  1.28s/it, epoch=0, global_step=251, lr=5e-5, step=5, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                    | 252/30000 [05:25<8:51:12,  1.07s/it, epoch=0, global_step=252, lr=5e-5, step=6, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                    | 253/30000 [05:26<7:38:50,  1.08it/s, epoch=0, global_step=253, lr=5e-5, step=7, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                    | 254/30000 [05:26<6:37:20,  1.25it/s, epoch=0, global_step=254, lr=5e-5, step=8, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                    | 255/30000 [05:27<6:13:44,  1.33it/s, epoch=0, global_step=255, lr=5e-5, step=9, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 256/30000 [05:28<5:51:01,  1.41it/s, epoch=0, global_step=256, lr=5e-5, step=10, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 257/30000 [05:28<5:33:32,  1.49it/s, epoch=0, global_step=257, lr=5e-5, step=11, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 258/30000 [05:29<5:20:26,  1.55it/s, epoch=0, global_step=258, lr=5e-5, step=12, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 259/30000 [05:29<5:20:30,  1.55it/s, epoch=0, global_step=259, lr=5e-5, step=13, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n",
      "Training Steps:   1%|                                   | 260/30000 [05:30<4:56:06,  1.67it/s, epoch=0, global_step=260, lr=5e-5, step=14, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 261/30000 [05:30<4:52:55,  1.69it/s, epoch=0, global_step=261, lr=5e-5, step=15, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 262/30000 [05:31<4:55:41,  1.68it/s, epoch=0, global_step=262, lr=5e-5, step=16, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 263/30000 [05:32<4:52:57,  1.69it/s, epoch=0, global_step=263, lr=5e-5, step=17, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 264/30000 [05:32<4:51:45,  1.70it/s, epoch=0, global_step=264, lr=5e-5, step=18, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 265/30000 [05:33<4:49:37,  1.71it/s, epoch=0, global_step=265, lr=5e-5, step=19, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 266/30000 [05:33<4:50:54,  1.70it/s, epoch=0, global_step=266, lr=5e-5, step=20, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 267/30000 [05:34<4:48:48,  1.72it/s, epoch=0, global_step=267, lr=5e-5, step=21, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 268/30000 [05:35<4:58:14,  1.66it/s, epoch=0, global_step=268, lr=5e-5, step=22, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 269/30000 [05:35<4:57:07,  1.67it/s, epoch=0, global_step=269, lr=5e-5, step=23, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 270/30000 [05:36<4:56:49,  1.67it/s, epoch=0, global_step=270, lr=5e-5, step=24, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 271/30000 [05:36<4:49:33,  1.71it/s, epoch=0, global_step=271, lr=5e-5, step=25, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 272/30000 [05:37<4:49:51,  1.71it/s, epoch=0, global_step=272, lr=5e-5, step=26, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 273/30000 [05:38<4:51:18,  1.70it/s, epoch=0, global_step=273, lr=5e-5, step=27, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 274/30000 [05:38<4:56:34,  1.67it/s, epoch=0, global_step=274, lr=5e-5, step=28, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 275/30000 [05:39<4:57:55,  1.66it/s, epoch=0, global_step=275, lr=5e-5, step=29, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 276/30000 [05:39<4:58:11,  1.66it/s, epoch=0, global_step=276, lr=5e-5, step=30, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Forward upsample size to force interpolation output size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   1%|                                   | 277/30000 [05:40<4:47:53,  1.72it/s, epoch=0, global_step=277, lr=5e-5, step=31, train_loss=1.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.7918, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m train_target_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mgenerator_step_M \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m*\u001b[39mclip_step_N\n\u001b[1;32m     13\u001b[0m cur_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Skip steps until we reach the resumed step\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mresume_from_checkpoint \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m first_epoch \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m<\u001b[39m resume_step:\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m training_args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/accelerate/data_loader.py:387\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 387\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/arrow_dataset.py:2782\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m   2781\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[1;32m   2784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/arrow_dataset.py:2778\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/arrow_dataset.py:2763\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2762\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 2763\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[1;32m   2765\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/formatting/formatting.py:624\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    622\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/formatting/formatting.py:400\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/datasets/formatting/formatting.py:510\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    508\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    509\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 510\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m, in \u001b[0;36mtransform_images\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# lambdalabs/pokemon-blip-captions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     images \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m examples[image_column]]\n\u001b[0;32m----> 9\u001b[0m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [image_transformations(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# lambdalabs/pokemon-blip-captions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     images \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m examples[image_column]]\n\u001b[0;32m----> 9\u001b[0m examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mimage_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m, in \u001b[0;36mTransform.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/conda/envs/pytorch2/lib/python3.8/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(first_epoch, training_args.num_train_epochs):\n",
    "    if training_args.do_train:\n",
    "        logging.info(\"*\"*50)\n",
    "        logging.info(\"Doing Training\")\n",
    "        logging.info(\"*\"*50)\n",
    "            \n",
    "        progress_bar.set_description(\"Training Steps\")\n",
    "        train_loss = 0.0\n",
    "\n",
    "        generator_step_M = 1\n",
    "        clip_step_N = 1\n",
    "        train_target_list = [\"generator\"]*generator_step_M + [\"clip\"]*clip_step_N\n",
    "        cur_index = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Skip steps until we reach the resumed step\n",
    "            if training_args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "                if step % training_args.gradient_accumulation_steps == 0:\n",
    "                    progress_bar.update(1)\n",
    "                continue\n",
    "            # which to train\n",
    "            train_target = train_target_list[cur_index]\n",
    "            cur_index = (cur_index + 1) % len(train_target_list)\n",
    "            if train_target == \"generator\":\n",
    "                pass\n",
    "\n",
    "            batch_pixel_values = batch[\"pixel_values\"]  # [6,3,224,224]\n",
    "            \n",
    "            batch_input_ids = batch[\"input_ids\"]\n",
    "            batch_attention_mask = batch[\"attention_mask\"]\n",
    "            print(batch_pixel_values.shape)\n",
    "            \n",
    "            if add_noise:\n",
    "                text_encoder = clip_model.text_model\n",
    "                with torch.no_grad():\n",
    "                    encoder_hidden_states = text_encoder(batch_input_ids,batch_attention_mask)[0]  # [6,128,768]                \n",
    "                noise = generator(batch_pixel_values, encoder_hidden_states)\n",
    "                \n",
    "                # limit the norm of the noise\n",
    "                norm_type = 'l2'\n",
    "                epsilon = 16\n",
    "                if norm_type == 'l2':\n",
    "                    temp = torch.norm(noise.view(noise.shape[0], -1), dim=1).view(-1, 1, 1, 1)\n",
    "                    noise = noise * epsilon / temp\n",
    "                else:\n",
    "                    noise = torch.clamp(noise, -epsilon / 255, epsilon / 255)\n",
    "                image = batch_pixel_values + noise \n",
    "                image = torch.clamp(image, -1, 1)\n",
    "            else:\n",
    "                image = batch_pixel_values\n",
    "                 \n",
    "            if use_normailize:\n",
    "                image = normalize_fn(image, mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "                \n",
    "            batch_data_input = {\n",
    "                \"input_ids\":batch_input_ids,\n",
    "                \"pixel_values\" : image,\n",
    "                \"attention_mask\":batch[\"attention_mask\"],\n",
    "                \"return_loss\": True\n",
    "            }\n",
    "            output = clip_model(**batch_data_input)\n",
    "            logits_per_image = output.logits_per_image   # for training , image_logits is the same as logits text\n",
    "            logits_per_text = output.logits_per_text\n",
    "            \n",
    "            loss = output.loss\n",
    "\n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            avg_loss = accelerator.gather(loss.repeat(training_args.train_batch_size)).mean()\n",
    "            train_loss += avg_loss.item() / training_args.gradient_accumulation_steps\n",
    "            print(avg_loss)\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                if generator_train:\n",
    "                    accelerator.clip_grad_norm_(generator.parameters(), training_args.max_grad_norm)\n",
    "                elif clip_train:\n",
    "                    accelerator.clip_grad_norm_(clip_model.parameters(), training_args.max_grad_norm)\n",
    "            \n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "            optimizer_generator.step()\n",
    "            # lr_scheduler.step()\n",
    "            \n",
    "            clip_model.zero_grad()\n",
    "            generator.zero_grad()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                # accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "                train_loss = 0.0\n",
    "\n",
    "                checkpointing_steps = 100\n",
    "                if global_step % checkpointing_steps == 0:\n",
    "                    logging.info(\"Epoch : {} ; Step : {} ; Save checkpoint to {}\".format(epoch, global_step, training_args.output_dir))\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(training_args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "            \n",
    "            record = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": step,\n",
    "                    \"global_step\":global_step,\n",
    "                    \"train_loss\": loss.detach().item(),\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "            # wandb.log(record)  \n",
    "            progress_bar.set_postfix(**record)\n",
    "\n",
    "            if global_step >= training_args.max_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81615e-e80a-4330-b928-26ec52ccc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38a5c2-b113-4ea2-9769-267837380d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544cd55-55b9-40d3-b4e2-2b63f4f868c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe778cc-ee4e-4174-8d2e-ed853b6beeb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ebbd1-dc69-4d83-9b21-53f1d561957c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
