{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b309fd4-fd9f-423e-a4be-1667334e6dcf",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23d9ffa-58fe-4811-a1d3-513618dfe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torch.nn as nn\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry,ContextManagers\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2435a-90f6-4f9c-9e90-7021e3882656",
   "metadata": {},
   "source": [
    "## Initialize sh Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a1a5b-d34d-4d20-a5f1-c1b6a8931101",
   "metadata": {},
   "source": [
    "## Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffcb95c-dfc3-4c93-882f-b47bd82c0b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./clip-roberta-finetuned\n",
      "../clip-roberta\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    output_dir='./clip-roberta-finetuned',\n",
    "    model_name_or_path='../clip-roberta',\n",
    "    data_dir='/remote-home/songtianwei/research/diffusion_model_my/data',\n",
    "    dataset_name='ydshieh/coco_dataset_script',\n",
    "    dataset_config_name='2017',\n",
    "    image_column='image_path',\n",
    "    caption_column='caption',\n",
    "    remove_unused_columns=False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size='64',\n",
    "    per_device_eval_batch_size='64',\n",
    "    learning_rate='5e-5',\n",
    "    warmup_steps='0',\n",
    "    weight_decay=0.1,\n",
    "    overwrite_output_dir=True,\n",
    "    input_perturbation=0.1,\n",
    "    dataset_noise_type='clip_min_noise',\n",
    "    dataset_normalize_flag=False,\n",
    "    max_train_samples=10000\n",
    ")\n",
    "\n",
    "# 使用示例\n",
    "print(args.output_dir)\n",
    "print(args.model_name_or_path)\n",
    "# 打印更多参数...\n",
    "\n",
    "# 在脚本中可以直接使用args变量来访问命令行参数的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c212fe7-7baa-4426-987f-0c5b7ae7cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '--output_dir', './clip-roberta-finetuned',\n",
    "    '--model_name_or_path', '/remote-home/songtianwei/research/diffusion_model_my/clip-roberta',\n",
    "    '--data_dir', '/remote-home/songtianwei/research/diffusion_model_my/data',\n",
    "    '--dataset_name', 'ydshieh/coco_dataset_script',\n",
    "    '--dataset_config_name', '2017',\n",
    "    '--image_column', 'image_path',\n",
    "    '--caption_column', 'caption',\n",
    "    '--remove_unused_columns', 'False',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    '--per_device_train_batch_size', '64',\n",
    "    '--per_device_eval_batch_size', '64',\n",
    "    '--learning_rate', '5e-5',\n",
    "    '--warmup_steps', '0',\n",
    "    '--weight_decay', '0.1',\n",
    "    '--overwrite_output_dir',\n",
    "    '--dataset_noise_type','clip_min_noise',\n",
    "    '--dataset_normalize_flag','False',\n",
    "    '--max_train_samples','10000',\n",
    "    '--report_to','wandb'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c6ec2-53c3-4fdc-8f99-81ab5551b39d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initilize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4e3dbd-9d8b-408c-8864-fa9ecbd6075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.32.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234cfd0d-d8ee-42f4-aa4c-0558d2598fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_vision_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n",
    "    )\n",
    "    freeze_text_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad35fec3-80cc-477c-b149-ccd1629b6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n",
    "    image_column: Optional[str] = field(\n",
    "        default=\"image_path\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n",
    "    )\n",
    "    caption_column: Optional[str] = field(\n",
    "        default=\"caption\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input testing data file (a jsonlines file).\"},\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    \n",
    "    # Noise type, default is none, other noise is \"random\" and \"clip_min_noise\"\n",
    "    dataset_noise_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The type of noise to add to the dataset.\"},\n",
    "    )\n",
    "    \n",
    "    dataset_normalize_flag: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to normalize the dataset.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension == \"json\", \"`validation_file` should be a json file.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd1219-f42b-431b-ba95-b167f59f9266",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## dataset name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb0625d-a9bd-47b8-b81d-99a6de35ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92294afc-e160-42e8-82e4-f4bf16d77ebe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c277af71-6822-44c0-84d4-149d80d30ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC,antialias=None),\n",
    "            CenterCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01437c-3218-400d-a051-e8b769339776",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c19475d-beff-454a-bf1d-8221305e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3660b34-6389-4b7f-b969-acea9fac7cf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6098a8fb-551e-4820-afda-6ff1037de887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.unet_config = {\n",
    "            \"act_fn\": \"silu\",\n",
    "            \"attention_head_dim\": 8,\n",
    "            \"block_out_channels\": [\n",
    "                320,\n",
    "                640,\n",
    "                1280,\n",
    "                1280\n",
    "            ],\n",
    "            \"center_input_sample\": False,\n",
    "            \"cross_attention_dim\": 768,\n",
    "            \"down_block_types\": [\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"DownBlock2D\"\n",
    "            ],\n",
    "            \"downsample_padding\": 1,\n",
    "            \"flip_sin_to_cos\": True,\n",
    "            \"freq_shift\": 0,\n",
    "            \"in_channels\": 4,\n",
    "            \"layers_per_block\": 2,\n",
    "            \"mid_block_scale_factor\": 1,\n",
    "            \"norm_eps\": 1e-05,\n",
    "            \"norm_num_groups\": 32,\n",
    "            \"out_channels\": 4,\n",
    "            \"sample_size\": 224,\n",
    "            \"up_block_types\": [\n",
    "                \"UpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\"\n",
    "            ]\n",
    "        }\n",
    "        self.unet = UNet2DConditionModel(**self.unet_config)\n",
    "        self.vae_config = {\n",
    "            'in_channels': 3,\n",
    "            'out_channels': 3,\n",
    "            'down_block_types': ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "            'up_block_types': ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "            'block_out_channels': [128, 256, 512, 512],\n",
    "            'layers_per_block': 2,\n",
    "            'act_fn': 'silu',\n",
    "            'latent_channels': 4,\n",
    "            'norm_num_groups': 32,\n",
    "            'sample_size': 512,\n",
    "            'scaling_factor': 0.18215,\n",
    "        }\n",
    "        self.vae = AutoencoderKL(**self.vae_config)\n",
    "        \n",
    "    def forward(self, img_pixel_values, encoder_hidden_states):\n",
    "        latent = self.vae.encode(img_pixel_values).latent_dist.sample()\n",
    "        timesteps = torch.randint(0, 1000, (1,),device=latent.device)\n",
    "        timesteps = timesteps.long()  #  6\n",
    "        unet_pred = self.unet(latent, timesteps, encoder_hidden_states).sample\n",
    "        vae_decoding = self.vae.decoder(unet_pred)\n",
    "        return vae_decoding\n",
    "    \n",
    "    def enable_xformers_memory_efficient_attention(self):\n",
    "        self.unet.enable_xformers_memory_efficient_attention()\n",
    "        self.vae.enable_xformers_memory_efficient_attention()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dcadf-092f-430e-b820-da0d3ce7f122",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Parse input arguments\r\n",
    "    # See all possible arguments in src/transformers/training_args.py\r\n",
    "    # or by passing the --help flag to this script.\r\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c8bbf5-3651-4ef7-8350-2841dcda181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    # If we pass only one argument to the script and it's the path to a json file,\n",
    "    # let's parse it to get our arguments.\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "else:\n",
    "    print(\"1\")\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=args_list)\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_clip\", model_args, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a88a85-c717-48dd-92e7-4e234d225fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wandb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.report_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf2b34-aeca-4188-bbfc-16d171a41bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe3859d-a1a9-4d83-927c-ce1f449f5e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:27:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "08/02/2023 14:27:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./clip-roberta-finetuned/runs/Aug02_14-27-40_00351cb6f98e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./clip-roberta-finetuned,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./clip-roberta-finetuned,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56937d-9b0c-426e-920f-d1c0e027c0b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.Initialize accelerator and distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5b6838-2181-4b07-8db3-27a70c0d7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:27:40 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(total_limit=training_args.save_total_limit)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "    mixed_precision=\"no\",\n",
    "    log_with=training_args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f4cc4-3ee8-4562-8a13-9c1c8e12d67a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## 4. Detecting last checkpoint and eventualy continue from last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41e623e-f8fb-4652-b6fa-69f8520ac144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb3f1a-672d-41b8-a708-a8adf63b4274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f02d884-9bf8-4ee4-91a5-e8a49800989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:27:40 - WARNING - datasets.load - Using the latest cached version of the module from /remote-home/songtianwei/.cache/huggingface/modules/datasets_modules/datasets/ydshieh--coco_dataset_script/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f (last modified on Mon Jun 26 20:20:20 2023) since it couldn't be found locally at ydshieh/coco_dataset_script., or remotely on the Hugging Face Hub.\n",
      "08/02/2023 14:27:40 - WARNING - datasets.builder - Found cached dataset coco_dataset_script (/remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad08de35ae2487aa63dbb3a6e85f63d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files this script will use the first column for the full image path and the second column for the\n",
    "# captions (unless you specify column names for this with the `image_column` and `caption_column` arguments).\n",
    "#\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    dataset = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        keep_in_memory=False,\n",
    "        data_dir=data_args.data_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.validation_file.split(\".\")[-1]\n",
    "    if data_args.test_file is not None:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "        extension = data_args.test_file.split(\".\")[-1]\n",
    "    dataset = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be6dec-ba9a-4ea1-8150-616428eb9c0f",
   "metadata": {},
   "source": [
    "## 6. Load pretrained model, tokenizer, and image processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8534f123-a3ee-4f45-8874-ac19389c9f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64ef6fb0-db38-4151-b2fa-f09fcc446a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09695a11-6475-496e-a6a0-8fcf63631a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/remote-home/songtianwei/research/diffusion_model_my/clip-roberta'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c985349-d79e-4744-9089-a1ae28283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb8166e-f412-4219-8564-e3dae58d3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_clip_tokenizer = True\n",
    "if use_clip_tokenizer:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=revision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2454ecd5-1947-4366-a89b-afa97900590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image_processor, in this script we only use this to get the mean and std for normalization.\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_args.image_processor_name or model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c633b2-1155-46b9-bc9d-d15a953d12a8",
   "metadata": {},
   "source": [
    "### clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b398b6c4-3326-4f13-a349-80d43f7b1d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clip_model = AutoModel.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "config = clip_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d206c09-d02f-46d4-b1ae-1560cb6dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_config = clip_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4944ea31-333b-4219-bc62-df2d29021397",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "869e44c4-6512-452e-abe2-e727ca468b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_pretrained:\n",
    "    clip_model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "else:\n",
    "    clip_model_config = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\").config\n",
    "    clip_model = AutoModel.from_config(clip_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65610027-e4e1-41a2-b11b-55adc4b4fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:02 - INFO - __main__ - clip_train: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clip_train = True\n",
    "logger.info(f\"clip_train: {clip_train}\")\n",
    "if clip_train:\n",
    "    clip_model.train()\n",
    "    clip_model.requires_grad_(True)\n",
    "else:\n",
    "    clip_model.eval()\n",
    "    clip_model.requires_grad_(False)\n",
    "    # _freeze_params(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fed037a-bc23-422c-9f90-eef52ce003f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _freeze_params(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if model_args.freeze_vision_model:\n",
    "    _freeze_params(clip_model.vision_model)\n",
    "\n",
    "if model_args.freeze_text_model:\n",
    "    _freeze_params(clip_model.text_model)\n",
    "\n",
    "if training_args.seed is not None:\n",
    "    set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906cd47-7032-4e1d-88f9-a2da85b8ebf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce78c80-c689-41b8-979e-47d8f2c78c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e21e44e8-dc3f-44eb-92c8-50d6dc03434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b2ffe33-1155-40dd-be89-62ef795bb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:10 - INFO - __main__ - generator_train: True\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"generator_train: {generator_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde20573-4d71-4711-867a-9011c70b5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator_train:\n",
    "    generator.train()\n",
    "    generator.requires_grad_(True)\n",
    "else:\n",
    "    generator.eval()\n",
    "    generator.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75fbd4-8454-4d63-8fba-aed91d053941",
   "metadata": {},
   "source": [
    "### text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "268dc764-89a9-4135-91a6-9e94556a16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepspeed_zero_init_disabled_context_manager():\n",
    "        \"\"\"\n",
    "        returns either a context list that includes one that will disable zero.Init or an empty context list\n",
    "        \"\"\"\n",
    "        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n",
    "        if deepspeed_plugin is None:\n",
    "            return []\n",
    "\n",
    "        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96734b8d-75bc-40ef-a720-2cb933d13e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=revision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edddc500-94e4-4119-a892-3c03e5907a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_encoder\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3624abb1-4077-4dd7-8fee-56b4f4c909d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "# as these models are only used for inference, keeping weights in full precision is not required.\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "# Move text_encode and vae to gpu and cast to weight_dtype\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3af7-bf46-4674-aa59-0caec2234a92",
   "metadata": {},
   "source": [
    "## 7. Get the column names for input/target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "039dca28-c8f8-4157-a3d9-a66d68e2769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "if training_args.do_train:\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = dataset[\"validation\"].column_names\n",
    "elif training_args.do_predict:\n",
    "    column_names = dataset[\"test\"].column_names\n",
    "else:\n",
    "    logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b81f2e-7dbf-4b97-8195-d062507cb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n",
    "if data_args.image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = data_args.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if data_args.caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = data_args.caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c6f5b-1a5c-41a0-9485-63762318eae5",
   "metadata": {},
   "source": [
    "## 8. Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46f47720-5543-44fc-a6e1-69cd4c1827ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_path'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9288390-f59e-49d2-baf3-610760b260e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcc987-1453-4e7f-98db-57a1777676f7",
   "metadata": {},
   "source": [
    "### tokenize_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8649bdf-32f4-4f02-aaa0-7cc4f1254a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args.max_seq_length = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6b63236-4f9e-4379-9359-e35363e5ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples):\n",
    "    captions = list(examples[caption_column])\n",
    "    text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be3e40-692b-4442-bed2-3231fede4f77",
   "metadata": {},
   "source": [
    "### transform_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d63887b-6cb6-426f-a2dd-0b25f14f8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_images(examples):\n",
    "    images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b852ada-5ba8-4dde-940a-ad4cfbe1c9ad",
   "metadata": {},
   "source": [
    "### filter_corrupt_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d2a3adef-58c4-42c2-b936-a4d8aa724c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_corrupt_images(examples):\n",
    "    \"\"\"remove problematic images\"\"\"\n",
    "    valid_images = []\n",
    "    for image_file in examples[image_column]:\n",
    "        try:\n",
    "            Image.open(image_file)\n",
    "            valid_images.append(True)\n",
    "        except Exception:\n",
    "            valid_images.append(False)\n",
    "    return valid_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266c149-5dcb-4999-a909-e2bdb2b18e5b",
   "metadata": {},
   "source": [
    "### do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f3ccb99-f700-48d1-91c1-ddd9b28e8fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:17 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-b058122c12e7928a.arrow\n",
      "08/02/2023 14:28:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-e2ce9860ae98d5ed.arrow\n",
      "08/02/2023 14:28:18 - WARNING - datasets.fingerprint - Parameter 'transform'=<function transform_images at 0x7f16d98ef040> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_train:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"train\" not in dataset:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "        # print(len(train_dataset))\n",
    "        train_dataset = train_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        \n",
    "        train_dataset = train_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        train_dataset.set_transform(transform_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30f5e5e4-49fa-4785-a63b-81c1a7d0739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_path', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 8212\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "892c3967-397d-457a-abb0-4d316a65188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,  # here change to False to check the order of the images\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=training_args.train_batch_size,\n",
    "    num_workers=training_args.dataloader_num_workers,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceee9c-5683-4a33-b387-5d5c369d34b7",
   "metadata": {},
   "source": [
    "### do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fec716e7-cc64-49e2-a274-51c61ffe6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-df5777364e1d24b5.arrow\n",
      "08/02/2023 14:28:20 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-6125ab8beee00b01.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_eval:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"validation\" not in dataset:\n",
    "            raise ValueError(\"--do_eval requires a train validation\")\n",
    "        eval_dataset = dataset[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        eval_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0de79590-485c-4880-984a-c131cb1093d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluation dataloader\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=training_args.eval_batch_size,\n",
    "    num_workers=training_args.dataloader_num_workers,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71221094-a8f5-42ba-a4fc-7b30e8159769",
   "metadata": {},
   "source": [
    "### do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a79ce7d-26e7-4d07-b80a-e87f010893be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.do_predict:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"test\" not in dataset:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)\n",
    "            test_dataset = test_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        test_dataset = test_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        test_dataset = test_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        test_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c12de7-0e67-4d0c-b31d-4c7da0aa8e85",
   "metadata": {},
   "source": [
    "### Normailze fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "960c175b-86d2-4911-af5a-9a5014bf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fn(x, mean, std):\n",
    "    return transforms.Normalize(mean=mean, std=std)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3e7b3-290b-4922-bd5d-a57ca42653da",
   "metadata": {},
   "source": [
    "## 9.Initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25082c8f-a4b6-49ba-8fee-95bdd96b935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "use_8bit_adam = False\n",
    "if use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "        )\n",
    "\n",
    "    optimizer_cls = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_cls = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8358e129-4555-4631-bc97-963082540582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adamw.AdamW"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63efdce7-4e75-4094-9aac-ec9596ba3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(clip_model, ALL_LAYERNORM_LAYERS)\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.1,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "optimizer = optimizer_cls(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1472ae2-0853-474c-a0a6-59384da85567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_model.embeddings.token_embedding.weight',\n",
       " 'text_model.embeddings.position_embedding.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'vision_model.embeddings.patch_embedding.weight',\n",
       " 'vision_model.embeddings.position_embedding.weight',\n",
       " 'vision_model.embeddings.class_embedding',\n",
       " 'vision_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'visual_projection.weight',\n",
       " 'text_projection.weight',\n",
       " 'logit_scale']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf56cf06-957f-4ebc-9e18-1db5b19c0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = 'linear'\n",
    "lr_warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f0cf978-d9c5-4984-99f9-023fb0420263",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=lr_warmup_steps * training_args.gradient_accumulation_steps,\n",
    "        num_training_steps=training_args.max_steps * training_args.gradient_accumulation_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e112dc4f-0960-4793-8822-246050ba7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo Optimizer for generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3b172-b841-4a06-8068-110c50e3d147",
   "metadata": {},
   "source": [
    "## 10.Initial About the accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0cdfd78-0bd1-45b0-9d3d-cc19a8e58513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cfb4d7f3-7455-411e-b78e-4532a34f1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./clip-roberta-finetuned'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77112bfd-b1d4-4daf-8e6e-8334386dc011",
   "metadata": {},
   "source": [
    "### load model and optimizer and dataloader to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd8c8ae4-634c-44a8-a3fe-88ea1fc01a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = accelerator.prepare(optimizer)\n",
    "lr_scheduler = accelerator.prepare(lr_scheduler)\n",
    "generator = accelerator.prepare(generator)\n",
    "clip_model = accelerator.prepare(clip_model)\n",
    "    \n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "eval_dataloader = accelerator.prepare(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7a513a2-8986-4c55-ad0f-6e7df4d9fd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34bad-acca-4227-8d72-fd702340ce15",
   "metadata": {},
   "source": [
    "## 11.Initialize max_train_step and tracker ( wandb start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e75bed4-71a1-4392-8f5f-153f464a91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / training_args.gradient_accumulation_steps)\n",
    "if training_args.max_steps is None or training_args.max_steps <= 0:\n",
    "    training_args.max_steps = training_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "training_args.max_steps = (int)(training_args.max_steps)\n",
    "training_args.num_train_epochs = (int)(training_args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dca957db-1862-4ec5-96bc-caed0b3f4fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18a367ca-70e1-41f4-a813-c6dbf7bac805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fe3c937-4085-4d32-8693-9139c025413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_project_name = \"text2image-fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3258147f-8ba0-440b-8239-171a71215133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:22 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2997155472song\u001b[0m (\u001b[33mawyl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2502a00c4ad544ada0649bd6a8a1bb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669420432299374, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/remote-home/songtianwei/research/diffusion_model_my/clip_train/wandb/run-20230802_142829-rh9voh7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q' target=\"_blank\">flowing-forest-183</a></strong> to <a href='https://wandb.ai/awyl/text2image-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awyl/text2image-fine-tune' target=\"_blank\">https://wandb.ai/awyl/text2image-fine-tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q' target=\"_blank\">https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    tracker_config = dict(vars(args))\n",
    "    # tracker_config.pop(\"validation_prompts\")\n",
    "    accelerator.init_trackers(tracker_project_name, tracker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed935705-fb7f-4778-9dc9-29f0b286da4b",
   "metadata": {},
   "source": [
    "## 12. Start Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff753a-1391-45e4-a4ac-73813b3ddac7",
   "metadata": {},
   "source": [
    "### log training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4122351f-fd79-4114-ab77-a6fb0925b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:42 - INFO - __main__ - ***** Running training *****\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Training num examples = 8212\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Evaluation num examples = 25014\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Num Epochs = 3\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Instantaneous batch size per device = 64\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Total optimization steps = 384\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = training_args.train_batch_size * accelerator.num_processes * training_args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "if args.do_train:\n",
    "    logger.info(f\"  Training num examples = {len(train_dataset)}\")\n",
    "if args.do_eval:\n",
    "    logger.info(f\"  Evaluation num examples = {len(eval_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {training_args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {training_args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {training_args.max_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8fc25-ec5f-4a3e-8b74-dbc307d98096",
   "metadata": {},
   "source": [
    "### resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d487bd3-0d87-4ac2-b285-a1a2256ba0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially load in the weights and states from a previous save\n",
    "if training_args.resume_from_checkpoint:\n",
    "    if training_args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(training_args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "        resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66e4fd10-d54c-474b-8926-589b34a19dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|                                                                                                               | 0/384 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(global_step, training_args.max_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02cb61ef-66bb-4f40-91da-336f15a87fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78a164-b92e-41d1-9501-e304e19b9a26",
   "metadata": {},
   "source": [
    "### Training Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "180ec96c-ac22-46d6-a778-215b0d517a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-4)\n",
    "optimizer = accelerator.prepare(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6a86579-0c33-4163-a99d-44b0352807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214588f7-f75c-44d9-9a49-4072a69f2e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|                                                                                                               | 0/384 [00:00<?, ?it/s]/remote-home/songtianwei/conda/envs/pytorch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %580 : int = prim::profile_ivalue(%dtype)\n",
      " does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training Steps:   0%|                                                 | 0/384 [11:58<?, ?it/s, epoch=0, global_step=0, lr=0.0001, step=102, train_loss=4.12]"
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch, training_args.num_train_epochs):\n",
    "    if training_args.do_train:\n",
    "        # logging.info(\"*\"*50)\n",
    "        # logging.info(\"Doing Training\")\n",
    "        # logging.info(\"*\"*50)\n",
    "        # if generator_train:\n",
    "        #     generator.train()\n",
    "        # else:\n",
    "        #     generator.eval()\n",
    "            \n",
    "        # if clip_train:\n",
    "        #     clip_model.train()\n",
    "        # else:\n",
    "        #     clip_model.eval()\n",
    "            \n",
    "        progress_bar.set_description(\"Training Steps\")\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # generator_step_M = 1\n",
    "        # clip_step_N = 1\n",
    "        # train_target_list = [\"generator\"]*generator_step_M + [\"clip\"]*clip_step_N\n",
    "        # cur_index = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clip_model.train()\n",
    "            # Skip steps until we reach the resumed step\n",
    "            # if training_args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "            #     if step % training_args.gradient_accumulation_steps == 0:\n",
    "            #         progress_bar.update(1)\n",
    "            #     continue\n",
    "            # which to train\n",
    "            # train_target = train_target_list[cur_index]\n",
    "            # cur_index = (cur_index + 1) % len(train_target_list)\n",
    "            # if train_target == \"generator\":\n",
    "            #     pass\n",
    "\n",
    "            # Convert images to latent space\n",
    "            # img_pixel_values = batch[\"pixel_values\"]  # [6,3,224,224]\n",
    "            # # Get the text embedding for conditioning\n",
    "            # batch_token_ids = batch[\"input_ids\"]\n",
    "            \n",
    "            # generator.zero_grad()\n",
    "            \n",
    "\n",
    "            # generator_train = False\n",
    "            # if generator_train:\n",
    "            #     encoder_hidden_states = text_encoder(batch_token_ids)[0]  # [6,77,768]                \n",
    "            #     noise = generator(img_pixel_values, encoder_hidden_states)\n",
    "                \n",
    "            #     # limit the norm of the noise\n",
    "            #     norm_type = 'l2'\n",
    "            #     epsilon = 16\n",
    "            #     if norm_type == 'l2':\n",
    "            #         temp = torch.norm(noise.view(noise.shape[0], -1), dim=1).view(-1, 1, 1, 1)\n",
    "            #         noise = noise * epsilon / temp\n",
    "            #     else:\n",
    "            #         noise = torch.clamp(noise, -epsilon / 255, epsilon / 255)\n",
    "                    \n",
    "            #     add_noise = False\n",
    "            #     if add_noise:\n",
    "            #         image = img_pixel_values + noise\n",
    "            #     else:\n",
    "            #         image = img_pixel_values + noise * torch.tensor(0.0).to(noise.device)\n",
    "            # else:\n",
    "            #     image = img_pixel_values \n",
    "            # image = img_pixel_values \n",
    "            # image = torch.clamp(image, -1, 1)\n",
    "            \n",
    "            # use_normailize = False\n",
    "            # if use_normailize:\n",
    "            #     image = normalize_fn(image)\n",
    "                \n",
    "            # data_input = {\n",
    "            #     \"input_ids\":batch_token_ids,\n",
    "            #     \"pixel_values\" : image,\n",
    "            #     \"attention_mask\":batch[\"attention_mask\"],\n",
    "            #     \"return_loss\": True\n",
    "            # }\n",
    "            output = clip_model(**batch)\n",
    "            # logits_per_image = output.logits_per_image   # for training , image_logits is the same as logits text\n",
    "            # logits_per_text = output.logits_per_text\n",
    "            \n",
    "            loss = output.loss\n",
    "            \n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            # avg_loss = accelerator.gather(loss.repeat(training_args.train_batch_size)).mean()\n",
    "            # train_loss += avg_loss.item() / training_args.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            # if accelerator.sync_gradients:\n",
    "            #     if generator_train:\n",
    "            #         accelerator.clip_grad_norm_(generator.parameters(), training_args.max_grad_norm)\n",
    "            #     elif clip_train:\n",
    "            #         accelerator.clip_grad_norm_(clip_model.parameters(), training_args.max_grad_norm)\n",
    "            \n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "            # lr_scheduler.step()\n",
    "            \n",
    "            clip_model.zero_grad()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            # if accelerator.sync_gradients:\n",
    "            #     progress_bar.update(1)\n",
    "            #     global_step += 1\n",
    "            #     accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            #     train_loss = 0.0\n",
    "\n",
    "            #     checkpointing_steps = 100\n",
    "            #     if global_step % checkpointing_steps == 0:\n",
    "            #         logging.info(\"Epoch : {} ; Step : {} ; Save checkpoint to {}\".format(epoch, global_step, training_args.output_dir))\n",
    "            #         if accelerator.is_main_process:\n",
    "            #             save_path = os.path.join(training_args.output_dir, f\"checkpoint-{global_step}\")\n",
    "            #             accelerator.save_state(save_path)\n",
    "            #             logger.info(f\"Saved state to {save_path}\")\n",
    "            \n",
    "            record = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": step,\n",
    "                    \"global_step\":global_step,\n",
    "                    \"train_loss\": loss.detach().item(),\n",
    "                    # \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "            wandb.log(record)  \n",
    "            progress_bar.set_postfix(**record)\n",
    "\n",
    "            # if global_step >= training_args.max_steps:\n",
    "            #     break\n",
    "\n",
    "    # evaluation on the eval dataset\n",
    "    # training_args.do_eval = False\n",
    "    # if training_args.do_eval and accelerator.is_main_process:\n",
    "        \n",
    "    #     logging.info(\"*\"*50)\n",
    "    #     logging.info(\"Doing Evaluation\")\n",
    "    #     logging.info(\"*\"*50)\n",
    "    #     progress_bar.set_description(\"Evaluation Steps\")\n",
    "        \n",
    "    #     generator.eval()\n",
    "    #     clip_model.eval()\n",
    "        \n",
    "    #     eval_losses = []\n",
    "    #     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    #         with torch.no_grad():\n",
    "    #             # Convert images to latent space\n",
    "    #             img_pixel_values = batch[\"pixel_values\"].to(weight_dtype)  # [6,3,224,224]\n",
    "\n",
    "    #             # Get the text embedding for conditioning\n",
    "    #             batch_token_ids = batch[\"input_ids\"]\n",
    "    #             if generator_train:\n",
    "    #                 encoder_hidden_states = text_encoder(batch_token_ids)[0]  # [6,77,768]                \n",
    "    #                 noise = generator.forward(img_pixel_values, encoder_hidden_states)\n",
    "                    \n",
    "    #                 # limit the norm of the noise\n",
    "    #                 norm_type = 'l2'\n",
    "    #                 epsilon = 16\n",
    "    #                 if norm_type == 'l2':\n",
    "    #                     temp = torch.norm(noise.view(noise.shape[0], -1), dim=1).view(-1, 1, 1, 1)\n",
    "    #                     noise = noise * epsilon / temp\n",
    "    #                 else:\n",
    "    #                     noise = torch.clamp(noise, -epsilon / 255, epsilon / 255)\n",
    "                        \n",
    "    #                 add_noise = False\n",
    "    #                 if add_noise:\n",
    "    #                     image = img_pixel_values + noise\n",
    "    #                 else:\n",
    "    #                     image = img_pixel_values + noise * torch.tensor(0.0).to(noise.device)\n",
    "    #             else:\n",
    "    #                 image = img_pixel_values \n",
    "    #             image = torch.clamp(image, -1, 1)\n",
    "                \n",
    "    #             use_normailize = False\n",
    "    #             if use_normailize:\n",
    "    #                 image = normalize_fn(image)\n",
    "                  \n",
    "    #             data_input = {\n",
    "    #                 \"input_ids\":batch_token_ids,\n",
    "    #                 \"pixel_values\" : image\n",
    "    #             }\n",
    "    #             output = clip_model(**data_input, return_loss=True)\n",
    "    #             logits_per_image = output.logits_per_image   # for training , image_logits is the same as logits text\n",
    "    #             logits_per_text = output.logits_per_text\n",
    "                \n",
    "    #             loss = output.loss\n",
    "                \n",
    "    #             # END MY CODE\n",
    "    #         eval_losses.append(loss.detach().item())\n",
    "    #         # logs = {\"step\" : step,  \"lr\": lr_scheduler.get_last_lr()[0],\"eval_loss\": loss.detach().item(),}\n",
    "    #         # progress_bar.set_postfix(**logs)\n",
    "    #     eval_mean_loss = np.mean(eval_losses)\n",
    "    #     eval_record = {\n",
    "    #                 \"epoch\": epoch,\n",
    "    #                 \"global_step\":global_step,\n",
    "    #                 \"eval_avg_loss\": eval_mean_loss,\n",
    "    #                 }\n",
    "    #     wandb.log(eval_record)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81615e-e80a-4330-b928-26ec52ccc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf4e11-7f2d-42d3-b518-067108ac3570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc308ea4-286e-4a2c-bd8c-1e3368c4a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c1486-dd3a-4fff-9b8b-ecc257a205df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe07e3c-03ae-42b0-bc90-3667d7355c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45735-5a70-4790-aebd-40c895e72952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
