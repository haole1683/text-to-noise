{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b309fd4-fd9f-423e-a4be-1667334e6dcf",
   "metadata": {},
   "source": [
    "## Load package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23d9ffa-58fe-4811-a1d3-513618dfe9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, ToTensor\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry,ContextManagers\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.utils import is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2435a-90f6-4f9c-9e90-7021e3882656",
   "metadata": {},
   "source": [
    "## Initialize sh Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a1a5b-d34d-4d20-a5f1-c1b6a8931101",
   "metadata": {},
   "source": [
    "## Args "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffcb95c-dfc3-4c93-882f-b47bd82c0b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./clip-roberta-finetuned\n",
      "../clip-roberta\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    output_dir='./clip-roberta-finetuned',\n",
    "    model_name_or_path='../clip-roberta',\n",
    "    data_dir='/remote-home/songtianwei/research/diffusion_model_my/data',\n",
    "    dataset_name='ydshieh/coco_dataset_script',\n",
    "    dataset_config_name='2017',\n",
    "    image_column='image_path',\n",
    "    caption_column='caption',\n",
    "    remove_unused_columns=False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size='64',\n",
    "    per_device_eval_batch_size='64',\n",
    "    learning_rate='5e-5',\n",
    "    warmup_steps='0',\n",
    "    weight_decay=0.1,\n",
    "    overwrite_output_dir=True,\n",
    "    input_perturbation=0.1,\n",
    "    dataset_noise_type='clip_min_noise',\n",
    "    dataset_normalize_flag=False,\n",
    "    max_train_samples=10000\n",
    ")\n",
    "\n",
    "# 使用示例\n",
    "print(args.output_dir)\n",
    "print(args.model_name_or_path)\n",
    "# 打印更多参数...\n",
    "\n",
    "# 在脚本中可以直接使用args变量来访问命令行参数的值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c212fe7-7baa-4426-987f-0c5b7ae7cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_list = [\n",
    "    '--output_dir', './clip-roberta-finetuned',\n",
    "    '--model_name_or_path', '/remote-home/songtianwei/research/diffusion_model_my/clip-roberta',\n",
    "    '--data_dir', '/remote-home/songtianwei/research/diffusion_model_my/data',\n",
    "    '--dataset_name', 'ydshieh/coco_dataset_script',\n",
    "    '--dataset_config_name', '2017',\n",
    "    '--image_column', 'image_path',\n",
    "    '--caption_column', 'caption',\n",
    "    '--remove_unused_columns', 'False',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "    '--per_device_train_batch_size', '64',\n",
    "    '--per_device_eval_batch_size', '64',\n",
    "    '--learning_rate', '5e-5',\n",
    "    '--warmup_steps', '0',\n",
    "    '--weight_decay', '0.1',\n",
    "    '--overwrite_output_dir',\n",
    "    '--dataset_noise_type','clip_min_noise',\n",
    "    '--dataset_normalize_flag','False',\n",
    "    '--max_train_samples','10000',\n",
    "    '--report_to','wandb'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c6ec2-53c3-4fdc-8f99-81ab5551b39d",
   "metadata": {},
   "source": [
    "## Initilize Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4e3dbd-9d8b-408c-8864-fa9ecbd6075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__, log_level=\"INFO\")\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "# check_min_version(\"4.32.0.dev0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/contrastive-image-text/requirements.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "234cfd0d-d8ee-42f4-aa4c-0558d2598fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    freeze_vision_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n",
    "    )\n",
    "    freeze_text_model: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad35fec3-80cc-477c-b149-ccd1629b6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n",
    "    image_column: Optional[str] = field(\n",
    "        default=\"image_path\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n",
    "    )\n",
    "    caption_column: Optional[str] = field(\n",
    "        default=\"caption\",\n",
    "        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input testing data file (a jsonlines file).\"},\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    \n",
    "    # Noise type, default is none, other noise is \"random\" and \"clip_min_noise\"\n",
    "    dataset_noise_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The type of noise to add to the dataset.\"},\n",
    "    )\n",
    "    \n",
    "    dataset_normalize_flag: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to normalize the dataset.\"},\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension == \"json\", \"`validation_file` should be a json file.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd1219-f42b-431b-ba95-b167f59f9266",
   "metadata": {},
   "source": [
    "## dataset name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb0625d-a9bd-47b8-b81d-99a6de35ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name_mapping = {\n",
    "    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92294afc-e160-42e8-82e4-f4bf16d77ebe",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c277af71-6822-44c0-84d4-149d80d30ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean=None, std=None):\n",
    "        super().__init__()\n",
    "        self.transforms = transforms.Compose([\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC,antialias=None),\n",
    "            CenterCrop(image_size),  # CenterCrop is required because Resize doesn't ensure same output size\n",
    "            # ConvertImageDtype(torch.float),\n",
    "            ToTensor(), \n",
    "        ])\n",
    "        if mean is not None and std is not None:\n",
    "            self.transforms.transforms.append(Normalize(mean=mean, std=std))\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a01437c-3218-400d-a051-e8b769339776",
   "metadata": {},
   "source": [
    "## Collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c19475d-beff-454a-bf1d-8221305e62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3660b34-6389-4b7f-b969-acea9fac7cf8",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6098a8fb-551e-4820-afda-6ff1037de887",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.unet_config = {\n",
    "            \"act_fn\": \"silu\",\n",
    "            \"attention_head_dim\": 8,\n",
    "            \"block_out_channels\": [\n",
    "                320,\n",
    "                640,\n",
    "                1280,\n",
    "                1280\n",
    "            ],\n",
    "            \"center_input_sample\": False,\n",
    "            \"cross_attention_dim\": 768,\n",
    "            \"down_block_types\": [\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"DownBlock2D\"\n",
    "            ],\n",
    "            \"downsample_padding\": 1,\n",
    "            \"flip_sin_to_cos\": True,\n",
    "            \"freq_shift\": 0,\n",
    "            \"in_channels\": 4,\n",
    "            \"layers_per_block\": 2,\n",
    "            \"mid_block_scale_factor\": 1,\n",
    "            \"norm_eps\": 1e-05,\n",
    "            \"norm_num_groups\": 32,\n",
    "            \"out_channels\": 4,\n",
    "            \"sample_size\": 224,\n",
    "            \"up_block_types\": [\n",
    "                \"UpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\"\n",
    "            ]\n",
    "        }\n",
    "        self.unet = UNet2DConditionModel(**self.unet_config)\n",
    "        self.vae_config = {\n",
    "            'in_channels': 3,\n",
    "            'out_channels': 3,\n",
    "            'down_block_types': ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "            'up_block_types': ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "            'block_out_channels': [128, 256, 512, 512],\n",
    "            'layers_per_block': 2,\n",
    "            'act_fn': 'silu',\n",
    "            'latent_channels': 4,\n",
    "            'norm_num_groups': 32,\n",
    "            'sample_size': 512,\n",
    "            'scaling_factor': 0.18215,\n",
    "        }\n",
    "        self.vae = AutoencoderKL(**self.vae_config)\n",
    "        \n",
    "    def forward(self, img_pixel_values, encoder_hidden_states):\n",
    "        latent = self.vae.encode(img_pixel_values).latent_dist.sample()\n",
    "        timesteps = torch.randint(0, 1000, (1,),device=latent.device)\n",
    "        timesteps = timesteps.long()  #  6\n",
    "        unet_pred = self.unet(latent, timesteps, encoder_hidden_states).sample\n",
    "        vae_decoding = self.vae.decoder(unet_pred)\n",
    "        return vae_decoding\n",
    "    \n",
    "    def enable_xformers_memory_efficient_attention(self):\n",
    "        self.unet.enable_xformers_memory_efficient_attention()\n",
    "        self.vae.enable_xformers_memory_efficient_attention()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2dcadf-092f-430e-b820-da0d3ce7f122",
   "metadata": {},
   "source": [
    "## 1. Parse input arguments\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2c8bbf5-3651-4ef7-8350-2841dcda181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "    # If we pass only one argument to the script and it's the path to a json file,\n",
    "    # let's parse it to get our arguments.\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "else:\n",
    "    print(\"1\")\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses(args=args_list)\n",
    "\n",
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "send_example_telemetry(\"run_clip\", model_args, data_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16a88a85-c717-48dd-92e7-4e234d225fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wandb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.report_to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf2b34-aeca-4188-bbfc-16d171a41bff",
   "metadata": {},
   "source": [
    "## 2. Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afe3859d-a1a9-4d83-927c-ce1f449f5e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:08:19 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "08/03/2023 20:08:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./clip-roberta-finetuned/runs/Aug03_20-08-18_00351cb6f98e,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=./clip-roberta-finetuned,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=64,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./clip-roberta-finetuned,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2. Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "if training_args.should_log:\n",
    "    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "log_level = training_args.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process the small summary:\n",
    "logger.warning(\n",
    "    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "    + f\"distributed training: {training_args.parallel_mode.value == 'distributed'}, 16-bits training: {training_args.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56937d-9b0c-426e-920f-d1c0e027c0b1",
   "metadata": {},
   "source": [
    "## 3.Initialize accelerator and distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5b6838-2181-4b07-8db3-27a70c0d7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:08:19 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accelerator_project_config = ProjectConfiguration(total_limit=training_args.save_total_limit)\n",
    "\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args.gradient_accumulation_steps,\n",
    "    mixed_precision=\"no\",\n",
    "    log_with=training_args.report_to,\n",
    "    project_config=accelerator_project_config,\n",
    ")\n",
    "\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_warning()\n",
    "    diffusers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    diffusers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f4cc4-3ee8-4562-8a13-9c1c8e12d67a",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Detecting last checkpoint and eventualy continue from last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41e623e-f8fb-4652-b6fa-69f8520ac144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb3f1a-672d-41b8-a708-a8adf63b4274",
   "metadata": {},
   "source": [
    "## 5. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f02d884-9bf8-4ee4-91a5-e8a49800989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:08:19 - WARNING - datasets.load - Using the latest cached version of the module from /remote-home/songtianwei/.cache/huggingface/modules/datasets_modules/datasets/ydshieh--coco_dataset_script/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f (last modified on Mon Jun 26 20:20:20 2023) since it couldn't be found locally at ydshieh/coco_dataset_script., or remotely on the Hugging Face Hub.\n",
      "08/03/2023 20:08:19 - WARNING - datasets.builder - Found cached dataset coco_dataset_script (/remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc246abf4954ec0a84f854c0addf55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files this script will use the first column for the full image path and the second column for the\n",
    "# captions (unless you specify column names for this with the `image_column` and `caption_column` arguments).\n",
    "#\n",
    "if data_args.dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    dataset = load_dataset(\n",
    "        data_args.dataset_name,\n",
    "        data_args.dataset_config_name,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        keep_in_memory=False,\n",
    "        data_dir=data_args.data_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "else:\n",
    "    data_files = {}\n",
    "    if data_args.train_file is not None:\n",
    "        data_files[\"train\"] = data_args.train_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "    if data_args.validation_file is not None:\n",
    "        data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.validation_file.split(\".\")[-1]\n",
    "    if data_args.test_file is not None:\n",
    "        data_files[\"test\"] = data_args.test_file\n",
    "        extension = data_args.test_file.split(\".\")[-1]\n",
    "    dataset = load_dataset(\n",
    "        extension,\n",
    "        data_files=data_files,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46be6dec-ba9a-4ea1-8150-616428eb9c0f",
   "metadata": {},
   "source": [
    "## 6. Load pretrained model, tokenizer, and image processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8534f123-a3ee-4f45-8874-ac19389c9f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if model_args.tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "elif model_args.model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64ef6fb0-db38-4151-b2fa-f09fcc446a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "revision = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09695a11-6475-496e-a6a0-8fcf63631a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/remote-home/songtianwei/research/diffusion_model_my/clip-roberta'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args.model_name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c985349-d79e-4744-9089-a1ae28283bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"CompVis/stable-diffusion-v1-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb8166e-f412-4219-8564-e3dae58d3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_clip_tokenizer = True\n",
    "if use_clip_tokenizer:\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"tokenizer\", revision=revision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2454ecd5-1947-4366-a89b-afa97900590b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load image_processor, in this script we only use this to get the mean and std for normalization.\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    model_args.image_processor_name or model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c633b2-1155-46b9-bc9d-d15a953d12a8",
   "metadata": {},
   "source": [
    "### clip_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b398b6c4-3326-4f13-a349-80d43f7b1d18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clip_model = AutoModel.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "config = clip_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d206c09-d02f-46d4-b1ae-1560cb6dc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_config = clip_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4944ea31-333b-4219-bc62-df2d29021397",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "869e44c4-6512-452e-abe2-e727ca468b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clip_pretrained:\n",
    "    clip_model = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "else:\n",
    "    clip_model_config = AutoModel.from_pretrained(\"openai/clip-vit-base-patch32\").config\n",
    "    clip_model = AutoModel.from_config(clip_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65610027-e4e1-41a2-b11b-55adc4b4fe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:08:39 - INFO - __main__ - clip_train: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clip_train = True\n",
    "logger.info(f\"clip_train: {clip_train}\")\n",
    "if clip_train:\n",
    "    clip_model.train()\n",
    "    clip_model.requires_grad_(True)\n",
    "else:\n",
    "    clip_model.eval()\n",
    "    clip_model.requires_grad_(False)\n",
    "    # _freeze_params(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fed037a-bc23-422c-9f90-eef52ce003f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _freeze_params(module):\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "if model_args.freeze_vision_model:\n",
    "    _freeze_params(clip_model.vision_model)\n",
    "\n",
    "if model_args.freeze_text_model:\n",
    "    _freeze_params(clip_model.text_model)\n",
    "\n",
    "if training_args.seed is not None:\n",
    "    set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906cd47-7032-4e1d-88f9-a2da85b8ebf3",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ce78c80-c689-41b8-979e-47d8f2c78c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_train=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e21e44e8-dc3f-44eb-92c8-50d6dc03434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b2ffe33-1155-40dd-be89-62ef795bb28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:08:46 - INFO - __main__ - generator_train: True\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"generator_train: {generator_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cde20573-4d71-4711-867a-9011c70b5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generator_train:\n",
    "    generator.train()\n",
    "    generator.requires_grad_(True)\n",
    "else:\n",
    "    generator.eval()\n",
    "    generator.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75fbd4-8454-4d63-8fba-aed91d053941",
   "metadata": {},
   "source": [
    "### text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "268dc764-89a9-4135-91a6-9e94556a16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepspeed_zero_init_disabled_context_manager():\n",
    "        \"\"\"\n",
    "        returns either a context list that includes one that will disable zero.Init or an empty context list\n",
    "        \"\"\"\n",
    "        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None\n",
    "        if deepspeed_plugin is None:\n",
    "            return []\n",
    "\n",
    "        return [deepspeed_plugin.zero3_init_context_manager(enable=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96734b8d-75bc-40ef-a720-2cb933d13e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ContextManagers(deepspeed_zero_init_disabled_context_manager()):\n",
    "    text_encoder = CLIPTextModel.from_pretrained(\n",
    "        pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=revision\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edddc500-94e4-4119-a892-3c03e5907a88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_encoder\n",
    "text_encoder.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3624abb1-4077-4dd7-8fee-56b4f4c909d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "# as these models are only used for inference, keeping weights in full precision is not required.\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "# Move text_encode and vae to gpu and cast to weight_dtype\n",
    "text_encoder.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e3af7-bf46-4674-aa59-0caec2234a92",
   "metadata": {},
   "source": [
    "## 7. Get the column names for input/target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "039dca28-c8f8-4157-a3d9-a66d68e2769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize inputs and targets.\n",
    "if training_args.do_train:\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = dataset[\"validation\"].column_names\n",
    "elif training_args.do_predict:\n",
    "    column_names = dataset[\"test\"].column_names\n",
    "else:\n",
    "    logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43b81f2e-7dbf-4b97-8195-d062507cb86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n",
    "if data_args.image_column is None:\n",
    "    image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n",
    "else:\n",
    "    image_column = data_args.image_column\n",
    "    if image_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n",
    "if data_args.caption_column is None:\n",
    "    caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n",
    "else:\n",
    "    caption_column = data_args.caption_column\n",
    "    if caption_column not in column_names:\n",
    "        raise ValueError(\n",
    "            f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c6f5b-1a5c-41a0-9485-63762318eae5",
   "metadata": {},
   "source": [
    "## 8. Preprocessing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "46f47720-5543-44fc-a6e1-69cd4c1827ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'image_path'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9288390-f59e-49d2-baf3-610760b260e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size\n",
    ")\n",
    "# image_transformations = torch.jit.script(image_transformations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcc987-1453-4e7f-98db-57a1777676f7",
   "metadata": {},
   "source": [
    "### tokenize_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6b63236-4f9e-4379-9359-e35363e5ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# We need to tokenize input captions and transform the images.\n",
    "def tokenize_captions(examples):\n",
    "    captions = list(examples[caption_column])\n",
    "    text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n",
    "    examples[\"input_ids\"] = text_inputs.input_ids\n",
    "    examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be3e40-692b-4442-bed2-3231fede4f77",
   "metadata": {},
   "source": [
    "### transform_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9d63887b-6cb6-426f-a2dd-0b25f14f8886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_images(examples):\n",
    "        if isinstance(examples[image_column][0],str):\n",
    "            # For coco dataset, the images are loaded as path\n",
    "            # images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "            images = [Image.open(image_file).convert(\"RGB\") for image_file in examples[image_column]]\n",
    "        else:\n",
    "            # lambdalabs/pokemon-blip-captions\n",
    "            images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b852ada-5ba8-4dde-940a-ad4cfbe1c9ad",
   "metadata": {},
   "source": [
    "### filter_corrupt_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2a3adef-58c4-42c2-b936-a4d8aa724c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_corrupt_images(examples):\n",
    "    \"\"\"remove problematic images\"\"\"\n",
    "    valid_images = []\n",
    "    for image_file in examples[image_column]:\n",
    "        try:\n",
    "            Image.open(image_file)\n",
    "            valid_images.append(True)\n",
    "        except Exception:\n",
    "            valid_images.append(False)\n",
    "    return valid_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266c149-5dcb-4999-a909-e2bdb2b18e5b",
   "metadata": {},
   "source": [
    "### do_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f3ccb99-f700-48d1-91c1-ddd9b28e8fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/03/2023 20:11:30 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-b058122c12e7928a.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/8212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if training_args.do_train:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"train\" not in dataset:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "            train_dataset = train_dataset.select(range(max_train_samples))\n",
    "        # print(len(train_dataset))\n",
    "        train_dataset = train_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        \n",
    "        train_dataset = train_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        train_dataset.set_transform(transform_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65103fe4-cb79-4144-ba80-3819f9cbe31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_path': '/remote-home/songtianwei/.cache/huggingface/datasets/downloads/extracted/2eaa9620da3c5978de7bf66708380d3175d70ab7bb370443574b53d85fe1ebf0/train2017/000000203564.jpg', 'input_ids': [49406, 320, 11652, 18125, 593, 320, 6716, 601, 518, 2184, 6744, 269, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'pixel_values': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])}\n"
     ]
    }
   ],
   "source": [
    "for data in train_dataset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30f5e5e4-49fa-4785-a63b-81c1a7d0739d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image_path', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 8212\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "892c3967-397d-457a-abb0-4d316a65188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train dataloader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=False,  # here change to False to check the order of the images\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=training_args.train_batch_size,\n",
    "    num_workers=training_args.dataloader_num_workers,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "062a52ae-7c8f-4a6c-9db3-902e6daf6953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[0.8196, 0.8275, 0.8314,  ..., 0.1490, 0.1529, 0.1529],\n",
      "          [0.8314, 0.8431, 0.8510,  ..., 0.1490, 0.1569, 0.1569],\n",
      "          [0.8431, 0.8588, 0.8824,  ..., 0.1412, 0.1451, 0.1451],\n",
      "          ...,\n",
      "          [0.8000, 0.8039, 0.8000,  ..., 0.1843, 0.2000, 0.1843],\n",
      "          [0.7961, 0.8000, 0.8078,  ..., 0.2118, 0.2157, 0.1961],\n",
      "          [0.7882, 0.7882, 0.7922,  ..., 0.2784, 0.2667, 0.2353]],\n",
      "\n",
      "         [[0.7490, 0.7569, 0.7608,  ..., 0.1373, 0.1412, 0.1412],\n",
      "          [0.7608, 0.7804, 0.7843,  ..., 0.1373, 0.1451, 0.1451],\n",
      "          [0.7765, 0.7922, 0.8196,  ..., 0.1294, 0.1333, 0.1333],\n",
      "          ...,\n",
      "          [0.7373, 0.7373, 0.7333,  ..., 0.2235, 0.2196, 0.1961],\n",
      "          [0.7255, 0.7294, 0.7373,  ..., 0.2039, 0.1961, 0.1804],\n",
      "          [0.7255, 0.7216, 0.7216,  ..., 0.2039, 0.1922, 0.1882]],\n",
      "\n",
      "         [[0.6510, 0.6627, 0.6745,  ..., 0.1176, 0.1176, 0.1176],\n",
      "          [0.6745, 0.6941, 0.7020,  ..., 0.1098, 0.1176, 0.1176],\n",
      "          [0.6941, 0.7216, 0.7451,  ..., 0.0941, 0.0980, 0.1059],\n",
      "          ...,\n",
      "          [0.6471, 0.6471, 0.6431,  ..., 0.1020, 0.1098, 0.0902],\n",
      "          [0.6392, 0.6392, 0.6510,  ..., 0.0980, 0.0941, 0.0824],\n",
      "          [0.6314, 0.6392, 0.6392,  ..., 0.0980, 0.0941, 0.0902]]],\n",
      "\n",
      "\n",
      "        [[[0.5059, 0.5098, 0.5137,  ..., 0.7843, 0.7804, 0.7804],\n",
      "          [0.5059, 0.5059, 0.5137,  ..., 0.7922, 0.7843, 0.7765],\n",
      "          [0.4941, 0.4941, 0.5059,  ..., 0.7882, 0.7843, 0.7804],\n",
      "          ...,\n",
      "          [0.2667, 0.2745, 0.2784,  ..., 0.6039, 0.6000, 0.5882],\n",
      "          [0.2627, 0.2667, 0.2706,  ..., 0.5961, 0.5882, 0.5725],\n",
      "          [0.2588, 0.2588, 0.2627,  ..., 0.5843, 0.5725, 0.5529]],\n",
      "\n",
      "         [[0.6118, 0.6157, 0.6196,  ..., 0.7843, 0.7804, 0.7804],\n",
      "          [0.6118, 0.6157, 0.6196,  ..., 0.7922, 0.7843, 0.7765],\n",
      "          [0.6078, 0.6118, 0.6196,  ..., 0.7882, 0.7843, 0.7804],\n",
      "          ...,\n",
      "          [0.4078, 0.4157, 0.4196,  ..., 0.6353, 0.6314, 0.6275],\n",
      "          [0.4039, 0.4078, 0.4118,  ..., 0.6314, 0.6157, 0.6078],\n",
      "          [0.4000, 0.4000, 0.4039,  ..., 0.6196, 0.6078, 0.5961]],\n",
      "\n",
      "         [[0.7294, 0.7333, 0.7373,  ..., 0.7843, 0.7804, 0.7804],\n",
      "          [0.7294, 0.7333, 0.7373,  ..., 0.7922, 0.7843, 0.7765],\n",
      "          [0.7255, 0.7294, 0.7373,  ..., 0.7882, 0.7843, 0.7804],\n",
      "          ...,\n",
      "          [0.5333, 0.5412, 0.5451,  ..., 0.6863, 0.6824, 0.6745],\n",
      "          [0.5294, 0.5333, 0.5373,  ..., 0.6824, 0.6784, 0.6667],\n",
      "          [0.5255, 0.5255, 0.5294,  ..., 0.6784, 0.6667, 0.6588]]],\n",
      "\n",
      "\n",
      "        [[[0.5020, 0.4431, 0.3216,  ..., 0.6980, 0.6353, 0.5765],\n",
      "          [0.3294, 0.1804, 0.0549,  ..., 0.6627, 0.6196, 0.5686],\n",
      "          [0.0745, 0.1294, 0.3843,  ..., 0.6235, 0.5961, 0.5529],\n",
      "          ...,\n",
      "          [0.6235, 0.5922, 0.6627,  ..., 0.5490, 0.5765, 0.5686],\n",
      "          [0.6627, 0.6196, 0.6588,  ..., 0.5216, 0.5569, 0.5843],\n",
      "          [0.6863, 0.6667, 0.6510,  ..., 0.4980, 0.5333, 0.5608]],\n",
      "\n",
      "         [[0.5294, 0.5059, 0.4078,  ..., 0.7373, 0.6784, 0.6196],\n",
      "          [0.3922, 0.2627, 0.1176,  ..., 0.7020, 0.6549, 0.6039],\n",
      "          [0.1255, 0.1647, 0.4118,  ..., 0.6667, 0.6353, 0.5882],\n",
      "          ...,\n",
      "          [0.4667, 0.4471, 0.4941,  ..., 0.4980, 0.5294, 0.5098],\n",
      "          [0.5098, 0.4706, 0.4941,  ..., 0.4706, 0.5137, 0.5216],\n",
      "          [0.5255, 0.5216, 0.4902,  ..., 0.4392, 0.4824, 0.4980]],\n",
      "\n",
      "         [[0.5451, 0.5569, 0.4902,  ..., 0.7059, 0.6549, 0.6039],\n",
      "          [0.4902, 0.3373, 0.1490,  ..., 0.6784, 0.6431, 0.6078],\n",
      "          [0.1647, 0.1686, 0.3922,  ..., 0.6471, 0.6275, 0.5961],\n",
      "          ...,\n",
      "          [0.3804, 0.3765, 0.3725,  ..., 0.4196, 0.4471, 0.4275],\n",
      "          [0.4196, 0.3961, 0.3725,  ..., 0.3843, 0.4235, 0.4275],\n",
      "          [0.4275, 0.4314, 0.3843,  ..., 0.3529, 0.3843, 0.3922]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.8196, 0.8275, 0.8314,  ..., 0.1490, 0.1529, 0.1529],\n",
      "          [0.8314, 0.8431, 0.8510,  ..., 0.1490, 0.1569, 0.1569],\n",
      "          [0.8431, 0.8588, 0.8824,  ..., 0.1412, 0.1451, 0.1451],\n",
      "          ...,\n",
      "          [0.8000, 0.8039, 0.8000,  ..., 0.1843, 0.2000, 0.1843],\n",
      "          [0.7961, 0.8000, 0.8078,  ..., 0.2118, 0.2157, 0.1961],\n",
      "          [0.7882, 0.7882, 0.7922,  ..., 0.2784, 0.2667, 0.2353]],\n",
      "\n",
      "         [[0.7490, 0.7569, 0.7608,  ..., 0.1373, 0.1412, 0.1412],\n",
      "          [0.7608, 0.7804, 0.7843,  ..., 0.1373, 0.1451, 0.1451],\n",
      "          [0.7765, 0.7922, 0.8196,  ..., 0.1294, 0.1333, 0.1333],\n",
      "          ...,\n",
      "          [0.7373, 0.7373, 0.7333,  ..., 0.2235, 0.2196, 0.1961],\n",
      "          [0.7255, 0.7294, 0.7373,  ..., 0.2039, 0.1961, 0.1804],\n",
      "          [0.7255, 0.7216, 0.7216,  ..., 0.2039, 0.1922, 0.1882]],\n",
      "\n",
      "         [[0.6510, 0.6627, 0.6745,  ..., 0.1176, 0.1176, 0.1176],\n",
      "          [0.6745, 0.6941, 0.7020,  ..., 0.1098, 0.1176, 0.1176],\n",
      "          [0.6941, 0.7216, 0.7451,  ..., 0.0941, 0.0980, 0.1059],\n",
      "          ...,\n",
      "          [0.6471, 0.6471, 0.6431,  ..., 0.1020, 0.1098, 0.0902],\n",
      "          [0.6392, 0.6392, 0.6510,  ..., 0.0980, 0.0941, 0.0824],\n",
      "          [0.6314, 0.6392, 0.6392,  ..., 0.0980, 0.0941, 0.0902]]],\n",
      "\n",
      "\n",
      "        [[[0.7098, 0.6824, 0.6471,  ..., 0.4157, 0.3882, 0.4353],\n",
      "          [0.7098, 0.6941, 0.6549,  ..., 0.4118, 0.3922, 0.4353],\n",
      "          [0.7098, 0.6941, 0.6588,  ..., 0.4118, 0.3922, 0.4353],\n",
      "          ...,\n",
      "          [0.6745, 0.6392, 0.6078,  ..., 0.5333, 0.6078, 0.6353],\n",
      "          [0.6824, 0.6431, 0.6078,  ..., 0.5255, 0.6039, 0.6392],\n",
      "          [0.6824, 0.6431, 0.6000,  ..., 0.5255, 0.6078, 0.6392]],\n",
      "\n",
      "         [[0.6157, 0.5686, 0.5137,  ..., 0.3059, 0.2902, 0.3333],\n",
      "          [0.6196, 0.5725, 0.5255,  ..., 0.3098, 0.2941, 0.3294],\n",
      "          [0.6196, 0.5765, 0.5294,  ..., 0.3137, 0.2980, 0.3333],\n",
      "          ...,\n",
      "          [0.6078, 0.5176, 0.4549,  ..., 0.4431, 0.5333, 0.5647],\n",
      "          [0.6118, 0.5176, 0.4549,  ..., 0.4392, 0.5333, 0.5647],\n",
      "          [0.6157, 0.5255, 0.4549,  ..., 0.4392, 0.5333, 0.5647]],\n",
      "\n",
      "         [[0.4118, 0.3294, 0.2510,  ..., 0.1686, 0.1490, 0.1725],\n",
      "          [0.4118, 0.3333, 0.2627,  ..., 0.1686, 0.1490, 0.1725],\n",
      "          [0.4118, 0.3373, 0.2667,  ..., 0.1608, 0.1490, 0.1725],\n",
      "          ...,\n",
      "          [0.4745, 0.2980, 0.2000,  ..., 0.3059, 0.3961, 0.4431],\n",
      "          [0.4863, 0.2980, 0.1922,  ..., 0.2902, 0.3843, 0.4431],\n",
      "          [0.4941, 0.3059, 0.2039,  ..., 0.2863, 0.3843, 0.4510]]],\n",
      "\n",
      "\n",
      "        [[[0.7098, 0.6824, 0.6471,  ..., 0.4157, 0.3882, 0.4353],\n",
      "          [0.7098, 0.6941, 0.6549,  ..., 0.4118, 0.3922, 0.4353],\n",
      "          [0.7098, 0.6941, 0.6588,  ..., 0.4118, 0.3922, 0.4353],\n",
      "          ...,\n",
      "          [0.6745, 0.6392, 0.6078,  ..., 0.5333, 0.6078, 0.6353],\n",
      "          [0.6824, 0.6431, 0.6078,  ..., 0.5255, 0.6039, 0.6392],\n",
      "          [0.6824, 0.6431, 0.6000,  ..., 0.5255, 0.6078, 0.6392]],\n",
      "\n",
      "         [[0.6157, 0.5686, 0.5137,  ..., 0.3059, 0.2902, 0.3333],\n",
      "          [0.6196, 0.5725, 0.5255,  ..., 0.3098, 0.2941, 0.3294],\n",
      "          [0.6196, 0.5765, 0.5294,  ..., 0.3137, 0.2980, 0.3333],\n",
      "          ...,\n",
      "          [0.6078, 0.5176, 0.4549,  ..., 0.4431, 0.5333, 0.5647],\n",
      "          [0.6118, 0.5176, 0.4549,  ..., 0.4392, 0.5333, 0.5647],\n",
      "          [0.6157, 0.5255, 0.4549,  ..., 0.4392, 0.5333, 0.5647]],\n",
      "\n",
      "         [[0.4118, 0.3294, 0.2510,  ..., 0.1686, 0.1490, 0.1725],\n",
      "          [0.4118, 0.3333, 0.2627,  ..., 0.1686, 0.1490, 0.1725],\n",
      "          [0.4118, 0.3373, 0.2667,  ..., 0.1608, 0.1490, 0.1725],\n",
      "          ...,\n",
      "          [0.4745, 0.2980, 0.2000,  ..., 0.3059, 0.3961, 0.4431],\n",
      "          [0.4863, 0.2980, 0.1922,  ..., 0.2902, 0.3843, 0.4431],\n",
      "          [0.4941, 0.3059, 0.2039,  ..., 0.2863, 0.3843, 0.4510]]]]), 'input_ids': tensor([[49406,  4093,  2731,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  3975,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320, 36145,  ..., 49407, 49407, 49407],\n",
      "        ...,\n",
      "        [49406,   320,  1771,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  8470,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  1579,  ..., 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'return_loss': True}\n",
      "{'pixel_values': tensor([[[[0.1882, 0.2314, 0.2863,  ..., 0.2392, 0.2118, 0.1608],\n",
      "          [0.1804, 0.2392, 0.2902,  ..., 0.2353, 0.2039, 0.1098],\n",
      "          [0.2863, 0.1922, 0.2431,  ..., 0.1098, 0.0510, 0.1529],\n",
      "          ...,\n",
      "          [0.7294, 0.7216, 0.7216,  ..., 0.4902, 0.5098, 0.4941],\n",
      "          [0.7255, 0.7216, 0.7176,  ..., 0.4667, 0.4706, 0.4627],\n",
      "          [0.7255, 0.7333, 0.7255,  ..., 0.4706, 0.4824, 0.4941]],\n",
      "\n",
      "         [[0.4039, 0.4000, 0.4588,  ..., 0.3333, 0.2980, 0.2627],\n",
      "          [0.3451, 0.3686, 0.4353,  ..., 0.3608, 0.2706, 0.2549],\n",
      "          [0.4588, 0.3608, 0.3961,  ..., 0.2275, 0.2118, 0.2745],\n",
      "          ...,\n",
      "          [0.7255, 0.7333, 0.7255,  ..., 0.4235, 0.4392, 0.4471],\n",
      "          [0.7333, 0.7294, 0.7255,  ..., 0.3882, 0.4000, 0.4235],\n",
      "          [0.7294, 0.7412, 0.7333,  ..., 0.4000, 0.4118, 0.4353]],\n",
      "\n",
      "         [[0.2235, 0.2039, 0.2627,  ..., 0.1804, 0.1686, 0.0824],\n",
      "          [0.1686, 0.2314, 0.2941,  ..., 0.1922, 0.1490, 0.0667],\n",
      "          [0.2314, 0.2118, 0.2353,  ..., 0.0588, 0.0314, 0.0863],\n",
      "          ...,\n",
      "          [0.7451, 0.7451, 0.7373,  ..., 0.3765, 0.3725, 0.3529],\n",
      "          [0.7490, 0.7412, 0.7333,  ..., 0.3255, 0.3451, 0.3529],\n",
      "          [0.7490, 0.7490, 0.7412,  ..., 0.3294, 0.3451, 0.3804]]],\n",
      "\n",
      "\n",
      "        [[[0.0627, 0.0588, 0.0588,  ..., 0.0667, 0.0667, 0.0667],\n",
      "          [0.0549, 0.0549, 0.0549,  ..., 0.0706, 0.0706, 0.0706],\n",
      "          [0.0627, 0.0588, 0.0588,  ..., 0.0706, 0.0706, 0.0706],\n",
      "          ...,\n",
      "          [0.4196, 0.4196, 0.4275,  ..., 0.5804, 0.7882, 0.8784],\n",
      "          [0.4196, 0.4314, 0.4275,  ..., 0.8039, 0.8510, 0.8275],\n",
      "          [0.4235, 0.4157, 0.4235,  ..., 0.8039, 0.7961, 0.8039]],\n",
      "\n",
      "         [[0.0196, 0.0157, 0.0157,  ..., 0.0196, 0.0196, 0.0235],\n",
      "          [0.0118, 0.0118, 0.0118,  ..., 0.0157, 0.0118, 0.0157],\n",
      "          [0.0196, 0.0157, 0.0157,  ..., 0.0157, 0.0196, 0.0196],\n",
      "          ...,\n",
      "          [0.2471, 0.2471, 0.2549,  ..., 0.4431, 0.6941, 0.7961],\n",
      "          [0.2510, 0.2627, 0.2588,  ..., 0.6980, 0.7569, 0.7333],\n",
      "          [0.2627, 0.2549, 0.2627,  ..., 0.7137, 0.6902, 0.6980]],\n",
      "\n",
      "         [[0.0118, 0.0078, 0.0078,  ..., 0.0118, 0.0157, 0.0157],\n",
      "          [0.0039, 0.0039, 0.0039,  ..., 0.0118, 0.0078, 0.0118],\n",
      "          [0.0118, 0.0078, 0.0078,  ..., 0.0118, 0.0157, 0.0157],\n",
      "          ...,\n",
      "          [0.1569, 0.1569, 0.1647,  ..., 0.3098, 0.5765, 0.7098],\n",
      "          [0.1608, 0.1725, 0.1647,  ..., 0.5804, 0.6588, 0.6431],\n",
      "          [0.1765, 0.1686, 0.1765,  ..., 0.6118, 0.6157, 0.6039]]],\n",
      "\n",
      "\n",
      "        [[[0.4706, 0.6157, 0.6235,  ..., 0.0941, 0.0941, 0.0980],\n",
      "          [0.4471, 0.5961, 0.6118,  ..., 0.1098, 0.1098, 0.1059],\n",
      "          [0.4353, 0.5843, 0.5961,  ..., 0.1059, 0.1137, 0.1020],\n",
      "          ...,\n",
      "          [0.3608, 0.3294, 0.3451,  ..., 0.0510, 0.0314, 0.0353],\n",
      "          [0.3686, 0.3647, 0.3020,  ..., 0.0431, 0.0510, 0.0353],\n",
      "          [0.3725, 0.2706, 0.1686,  ..., 0.0314, 0.0471, 0.0431]],\n",
      "\n",
      "         [[0.6863, 0.6196, 0.6196,  ..., 0.2745, 0.2510, 0.2549],\n",
      "          [0.6941, 0.6275, 0.6275,  ..., 0.2745, 0.2549, 0.2627],\n",
      "          [0.6941, 0.6353, 0.6235,  ..., 0.2706, 0.2627, 0.2627],\n",
      "          ...,\n",
      "          [0.3647, 0.3490, 0.3608,  ..., 0.0706, 0.0667, 0.0392],\n",
      "          [0.3725, 0.3686, 0.3020,  ..., 0.0549, 0.0667, 0.0510],\n",
      "          [0.3725, 0.2627, 0.1569,  ..., 0.0275, 0.0471, 0.0588]],\n",
      "\n",
      "         [[0.8902, 0.8902, 0.8863,  ..., 0.2863, 0.2745, 0.2784],\n",
      "          [0.8824, 0.8824, 0.8824,  ..., 0.3020, 0.2863, 0.2745],\n",
      "          [0.8745, 0.8784, 0.8745,  ..., 0.2980, 0.2941, 0.2784],\n",
      "          ...,\n",
      "          [0.3686, 0.3647, 0.3843,  ..., 0.0863, 0.0863, 0.0314],\n",
      "          [0.3765, 0.3922, 0.3216,  ..., 0.0706, 0.0863, 0.0627],\n",
      "          [0.3804, 0.2824, 0.1686,  ..., 0.0431, 0.0627, 0.0745]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2157, 0.2000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1608, 0.1373, 0.0784],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.1608, 0.1098],\n",
      "          ...,\n",
      "          [0.0000, 0.0941, 0.2784,  ..., 0.0824, 0.0980, 0.0824],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0667, 0.0588, 0.0627],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0196, 0.0353]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2157, 0.2000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1608, 0.1373, 0.0784],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.1608, 0.1098],\n",
      "          ...,\n",
      "          [0.0000, 0.0941, 0.2784,  ..., 0.0824, 0.0980, 0.0824],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0667, 0.0588, 0.0627],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0196, 0.0353]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.2157, 0.2000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1608, 0.1373, 0.0784],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.2353, 0.1608, 0.1098],\n",
      "          ...,\n",
      "          [0.0000, 0.0941, 0.2784,  ..., 0.0824, 0.0980, 0.0824],\n",
      "          [0.0039, 0.0000, 0.0000,  ..., 0.0667, 0.0588, 0.0627],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0353, 0.0196, 0.0353]]],\n",
      "\n",
      "\n",
      "        [[[0.6549, 0.6745, 0.6235,  ..., 0.6471, 0.6275, 0.6431],\n",
      "          [0.2667, 0.2314, 0.2431,  ..., 0.2941, 0.2824, 0.2667],\n",
      "          [0.2431, 0.2510, 0.2745,  ..., 0.3216, 0.3412, 0.3294],\n",
      "          ...,\n",
      "          [0.2549, 0.2471, 0.2353,  ..., 0.1882, 0.2157, 0.1843],\n",
      "          [0.4588, 0.4667, 0.4627,  ..., 0.4549, 0.4627, 0.4627],\n",
      "          [0.9686, 0.9725, 0.9686,  ..., 0.9725, 0.9725, 0.9725]],\n",
      "\n",
      "         [[0.6784, 0.6980, 0.6431,  ..., 0.6863, 0.6667, 0.6863],\n",
      "          [0.3098, 0.2980, 0.3176,  ..., 0.3765, 0.3686, 0.3647],\n",
      "          [0.2941, 0.3294, 0.3569,  ..., 0.4157, 0.4196, 0.4039],\n",
      "          ...,\n",
      "          [0.2471, 0.2275, 0.2196,  ..., 0.1765, 0.1882, 0.1608],\n",
      "          [0.4588, 0.4588, 0.4627,  ..., 0.4588, 0.4588, 0.4588],\n",
      "          [0.9725, 0.9725, 0.9765,  ..., 0.9725, 0.9725, 0.9725]],\n",
      "\n",
      "         [[0.6157, 0.6392, 0.6039,  ..., 0.5961, 0.5961, 0.6118],\n",
      "          [0.2078, 0.1412, 0.1216,  ..., 0.1804, 0.1843, 0.1765],\n",
      "          [0.1608, 0.1608, 0.1608,  ..., 0.2431, 0.2510, 0.2157],\n",
      "          ...,\n",
      "          [0.2196, 0.2039, 0.2000,  ..., 0.1490, 0.1686, 0.1451],\n",
      "          [0.4549, 0.4588, 0.4588,  ..., 0.4431, 0.4471, 0.4549],\n",
      "          [0.9725, 0.9725, 0.9686,  ..., 0.9725, 0.9725, 0.9725]]],\n",
      "\n",
      "\n",
      "        [[[0.1059, 0.0745, 0.0784,  ..., 0.6157, 0.6196, 0.6196],\n",
      "          [0.1255, 0.0510, 0.0706,  ..., 0.6235, 0.6235, 0.6235],\n",
      "          [0.0784, 0.0314, 0.0549,  ..., 0.6314, 0.6235, 0.6235],\n",
      "          ...,\n",
      "          [0.7020, 0.7176, 0.6784,  ..., 0.7569, 0.7451, 0.7098],\n",
      "          [0.6902, 0.7059, 0.6902,  ..., 0.7529, 0.6941, 0.7098],\n",
      "          [0.6706, 0.6784, 0.6784,  ..., 0.7294, 0.7137, 0.7373]],\n",
      "\n",
      "         [[0.1255, 0.0902, 0.0863,  ..., 0.8235, 0.8275, 0.8275],\n",
      "          [0.1529, 0.0745, 0.0784,  ..., 0.8235, 0.8275, 0.8314],\n",
      "          [0.0863, 0.0471, 0.0627,  ..., 0.8353, 0.8235, 0.8235],\n",
      "          ...,\n",
      "          [0.6980, 0.7137, 0.6824,  ..., 0.7569, 0.7373, 0.7137],\n",
      "          [0.6863, 0.7059, 0.6941,  ..., 0.7412, 0.6902, 0.7059],\n",
      "          [0.6667, 0.6784, 0.6824,  ..., 0.7176, 0.7216, 0.7333]],\n",
      "\n",
      "         [[0.0667, 0.0549, 0.0588,  ..., 0.9961, 0.9961, 0.9922],\n",
      "          [0.0980, 0.0549, 0.0588,  ..., 0.9922, 0.9961, 0.9922],\n",
      "          [0.0824, 0.0275, 0.0588,  ..., 0.9961, 1.0000, 0.9922],\n",
      "          ...,\n",
      "          [0.6863, 0.6980, 0.6627,  ..., 0.7255, 0.7216, 0.6941],\n",
      "          [0.6627, 0.6863, 0.6706,  ..., 0.7333, 0.6824, 0.6902],\n",
      "          [0.6431, 0.6627, 0.6667,  ..., 0.7137, 0.6902, 0.7255]]]]), 'input_ids': tensor([[49406,  5560,  1047,  ..., 49407, 49407, 49407],\n",
      "        [49406,   786,   530,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  2863,  ..., 49407, 49407, 49407],\n",
      "        ...,\n",
      "        [49406,   550, 14789,  ..., 49407, 49407, 49407],\n",
      "        [49406, 11795,  3701,  ..., 49407, 49407, 49407],\n",
      "        [49406,  1237,  1507,  ..., 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'return_loss': True}\n",
      "{'pixel_values': tensor([[[[0.7137, 0.7216, 0.7255,  ..., 0.9882, 0.9882, 0.9686],\n",
      "          [0.7294, 0.7255, 0.7333,  ..., 0.9882, 0.9882, 0.9922],\n",
      "          [0.7608, 0.7922, 0.7882,  ..., 0.9922, 0.9922, 0.9961],\n",
      "          ...,\n",
      "          [0.1176, 0.1569, 0.1804,  ..., 0.1922, 0.1647, 0.1176],\n",
      "          [0.1137, 0.1569, 0.1725,  ..., 0.1765, 0.1451, 0.1098],\n",
      "          [0.1216, 0.1451, 0.1647,  ..., 0.1490, 0.1255, 0.0902]],\n",
      "\n",
      "         [[0.5451, 0.5490, 0.5451,  ..., 0.9843, 0.9569, 0.9137],\n",
      "          [0.5804, 0.5725, 0.5804,  ..., 0.9961, 0.9961, 0.9961],\n",
      "          [0.5765, 0.5843, 0.5843,  ..., 0.9922, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.0941, 0.1569, 0.1765,  ..., 0.1765, 0.1529, 0.1137],\n",
      "          [0.0980, 0.1529, 0.1686,  ..., 0.1608, 0.1294, 0.0980],\n",
      "          [0.1059, 0.1451, 0.1608,  ..., 0.1490, 0.1216, 0.0824]],\n",
      "\n",
      "         [[0.3725, 0.3765, 0.3765,  ..., 0.8627, 0.7569, 0.6549],\n",
      "          [0.3843, 0.3725, 0.3804,  ..., 0.9961, 1.0000, 1.0000],\n",
      "          [0.3882, 0.3725, 0.3843,  ..., 0.9922, 0.9922, 0.9961],\n",
      "          ...,\n",
      "          [0.0863, 0.1333, 0.1647,  ..., 0.1569, 0.1333, 0.0941],\n",
      "          [0.0902, 0.1373, 0.1608,  ..., 0.1451, 0.1137, 0.0824],\n",
      "          [0.1059, 0.1294, 0.1451,  ..., 0.1255, 0.1020, 0.0667]]],\n",
      "\n",
      "\n",
      "        [[[0.2353, 0.2039, 0.2039,  ..., 0.0078, 0.0078, 0.0078],\n",
      "          [0.3059, 0.2275, 0.1804,  ..., 0.0039, 0.0078, 0.0078],\n",
      "          [0.2471, 0.2980, 0.2667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.2471, 0.2745, 0.2627,  ..., 0.4863, 0.4745, 0.4784],\n",
      "          [0.2510, 0.2471, 0.2510,  ..., 0.5098, 0.5020, 0.5059],\n",
      "          [0.2000, 0.2510, 0.2471,  ..., 0.4902, 0.5020, 0.5059]],\n",
      "\n",
      "         [[0.2941, 0.2863, 0.3020,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3608, 0.3098, 0.2706,  ..., 0.0078, 0.0078, 0.0039],\n",
      "          [0.2980, 0.3529, 0.3216,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.3020, 0.3412, 0.3137,  ..., 0.5020, 0.4980, 0.4902],\n",
      "          [0.3059, 0.3098, 0.3020,  ..., 0.5176, 0.5098, 0.5098],\n",
      "          [0.2706, 0.3176, 0.3059,  ..., 0.5059, 0.5098, 0.5137]],\n",
      "\n",
      "         [[0.2157, 0.2196, 0.2392,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2941, 0.2549, 0.2157,  ..., 0.0039, 0.0078, 0.0078],\n",
      "          [0.2275, 0.2941, 0.2667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.2118, 0.2392, 0.2157,  ..., 0.4000, 0.4078, 0.4078],\n",
      "          [0.2196, 0.2157, 0.2118,  ..., 0.4196, 0.4235, 0.4235],\n",
      "          [0.1882, 0.2353, 0.2235,  ..., 0.4196, 0.4392, 0.4431]]],\n",
      "\n",
      "\n",
      "        [[[0.3020, 0.3647, 0.2392,  ..., 0.7529, 0.7451, 0.6392],\n",
      "          [0.2941, 0.4078, 0.3098,  ..., 0.7843, 0.7333, 0.7451],\n",
      "          [0.2824, 0.4314, 0.4196,  ..., 0.7725, 0.7529, 0.7725],\n",
      "          ...,\n",
      "          [0.4039, 0.1569, 0.1529,  ..., 0.4471, 0.4353, 0.4824],\n",
      "          [0.1569, 0.0667, 0.0941,  ..., 0.4667, 0.4784, 0.4627],\n",
      "          [0.0863, 0.0706, 0.0510,  ..., 0.5529, 0.5020, 0.4902]],\n",
      "\n",
      "         [[0.3020, 0.3451, 0.2392,  ..., 0.7216, 0.7333, 0.5882],\n",
      "          [0.2706, 0.3647, 0.2863,  ..., 0.7882, 0.7137, 0.7059],\n",
      "          [0.2510, 0.3804, 0.3725,  ..., 0.7686, 0.7490, 0.7725],\n",
      "          ...,\n",
      "          [0.3294, 0.1333, 0.1529,  ..., 0.4314, 0.4235, 0.4627],\n",
      "          [0.1373, 0.0863, 0.1059,  ..., 0.4392, 0.4588, 0.4392],\n",
      "          [0.0980, 0.0980, 0.0745,  ..., 0.5216, 0.4745, 0.4627]],\n",
      "\n",
      "         [[0.2745, 0.3176, 0.2235,  ..., 0.7098, 0.6941, 0.5373],\n",
      "          [0.2510, 0.3373, 0.2706,  ..., 0.7686, 0.6824, 0.6706],\n",
      "          [0.2314, 0.3490, 0.3451,  ..., 0.7529, 0.7216, 0.7412],\n",
      "          ...,\n",
      "          [0.0275, 0.1020, 0.1804,  ..., 0.4039, 0.4039, 0.4431],\n",
      "          [0.0824, 0.1176, 0.1412,  ..., 0.4157, 0.4353, 0.4275],\n",
      "          [0.1373, 0.1843, 0.1373,  ..., 0.5059, 0.4510, 0.4431]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.1686, 0.1608, 0.1608,  ..., 0.4157, 0.4196, 0.4078],\n",
      "          [0.1451, 0.1451, 0.1412,  ..., 0.4471, 0.4353, 0.4118],\n",
      "          [0.1294, 0.1294, 0.1255,  ..., 0.4588, 0.4549, 0.4235],\n",
      "          ...,\n",
      "          [0.2039, 0.2000, 0.1882,  ..., 0.7608, 0.7608, 0.7647],\n",
      "          [0.1882, 0.2000, 0.2000,  ..., 0.7569, 0.7569, 0.7608],\n",
      "          [0.1922, 0.2078, 0.1922,  ..., 0.7373, 0.7412, 0.7412]],\n",
      "\n",
      "         [[0.1451, 0.1451, 0.1412,  ..., 0.2667, 0.2627, 0.2627],\n",
      "          [0.1294, 0.1333, 0.1294,  ..., 0.2745, 0.2627, 0.2627],\n",
      "          [0.1176, 0.1137, 0.1137,  ..., 0.2745, 0.2706, 0.2667],\n",
      "          ...,\n",
      "          [0.2000, 0.1961, 0.1843,  ..., 0.7255, 0.7255, 0.7255],\n",
      "          [0.1804, 0.1922, 0.1922,  ..., 0.7216, 0.7216, 0.7216],\n",
      "          [0.1804, 0.1922, 0.1804,  ..., 0.7020, 0.7020, 0.7020]],\n",
      "\n",
      "         [[0.1216, 0.1216, 0.1216,  ..., 0.1882, 0.1961, 0.1922],\n",
      "          [0.1059, 0.1098, 0.1098,  ..., 0.2039, 0.2000, 0.1922],\n",
      "          [0.0980, 0.1020, 0.0980,  ..., 0.1961, 0.2039, 0.1961],\n",
      "          ...,\n",
      "          [0.2039, 0.2000, 0.1882,  ..., 0.7294, 0.7294, 0.7294],\n",
      "          [0.1882, 0.1961, 0.2078,  ..., 0.7255, 0.7255, 0.7255],\n",
      "          [0.1882, 0.2078, 0.1922,  ..., 0.7059, 0.7059, 0.7059]]],\n",
      "\n",
      "\n",
      "        [[[0.3333, 0.1490, 0.1176,  ..., 0.3608, 0.2314, 0.1686],\n",
      "          [0.3176, 0.2431, 0.1686,  ..., 0.3725, 0.3176, 0.2471],\n",
      "          [0.2745, 0.2471, 0.3725,  ..., 0.3686, 0.3725, 0.3608],\n",
      "          ...,\n",
      "          [0.4980, 0.4902, 0.4941,  ..., 0.1804, 0.1725, 0.1765],\n",
      "          [0.4863, 0.4902, 0.5020,  ..., 0.2627, 0.2549, 0.3451],\n",
      "          [0.4902, 0.4902, 0.5020,  ..., 0.4235, 0.4235, 0.4275]],\n",
      "\n",
      "         [[0.3412, 0.1569, 0.1255,  ..., 0.3255, 0.2000, 0.1451],\n",
      "          [0.3255, 0.2510, 0.1725,  ..., 0.3529, 0.2902, 0.2078],\n",
      "          [0.2510, 0.2549, 0.3922,  ..., 0.3529, 0.3569, 0.3333],\n",
      "          ...,\n",
      "          [0.7647, 0.7804, 0.7804,  ..., 0.0941, 0.0902, 0.1020],\n",
      "          [0.7686, 0.7804, 0.7843,  ..., 0.1725, 0.1804, 0.2706],\n",
      "          [0.7725, 0.7843, 0.7922,  ..., 0.3333, 0.3294, 0.3373]],\n",
      "\n",
      "         [[0.2980, 0.1176, 0.1020,  ..., 0.2235, 0.1255, 0.0980],\n",
      "          [0.2784, 0.2235, 0.1451,  ..., 0.2353, 0.2000, 0.1412],\n",
      "          [0.2039, 0.2275, 0.3529,  ..., 0.2431, 0.2431, 0.2275],\n",
      "          ...,\n",
      "          [0.7255, 0.7216, 0.7333,  ..., 0.0275, 0.0235, 0.0157],\n",
      "          [0.7176, 0.7216, 0.7333,  ..., 0.0706, 0.0745, 0.1373],\n",
      "          [0.7216, 0.7216, 0.7373,  ..., 0.1765, 0.1725, 0.1804]]],\n",
      "\n",
      "\n",
      "        [[[0.3725, 0.3608, 0.3608,  ..., 0.6039, 0.6157, 0.6471],\n",
      "          [0.3765, 0.3686, 0.3725,  ..., 0.6863, 0.7294, 0.7255],\n",
      "          [0.3765, 0.3686, 0.3647,  ..., 0.8078, 0.8157, 0.8667],\n",
      "          ...,\n",
      "          [0.6784, 0.7451, 0.6941,  ..., 0.6941, 0.6941, 0.6902],\n",
      "          [0.7529, 0.7647, 0.7294,  ..., 0.6941, 0.6902, 0.6863],\n",
      "          [0.7216, 0.7608, 0.7412,  ..., 0.6824, 0.6863, 0.6863]],\n",
      "\n",
      "         [[0.3686, 0.3569, 0.3569,  ..., 0.6431, 0.6667, 0.7020],\n",
      "          [0.3725, 0.3647, 0.3686,  ..., 0.7333, 0.7843, 0.7843],\n",
      "          [0.3725, 0.3647, 0.3608,  ..., 0.8667, 0.8824, 0.9216],\n",
      "          ...,\n",
      "          [0.6235, 0.7098, 0.6275,  ..., 0.6745, 0.6745, 0.6706],\n",
      "          [0.7216, 0.7412, 0.7137,  ..., 0.6706, 0.6706, 0.6667],\n",
      "          [0.6941, 0.7373, 0.7294,  ..., 0.6588, 0.6588, 0.6627]],\n",
      "\n",
      "         [[0.3490, 0.3451, 0.3490,  ..., 0.6824, 0.7020, 0.7255],\n",
      "          [0.3569, 0.3529, 0.3608,  ..., 0.7686, 0.8157, 0.8275],\n",
      "          [0.3529, 0.3529, 0.3569,  ..., 0.8941, 0.9059, 0.9412],\n",
      "          ...,\n",
      "          [0.5922, 0.6549, 0.5882,  ..., 0.6902, 0.6863, 0.6863],\n",
      "          [0.7059, 0.6980, 0.6627,  ..., 0.6863, 0.6863, 0.6824],\n",
      "          [0.6784, 0.6941, 0.6784,  ..., 0.6745, 0.6824, 0.6824]]]]), 'input_ids': tensor([[49406,   320,   786,  ..., 49407, 49407, 49407],\n",
      "        [49406, 21817,  4659,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  1857,  ..., 49407, 49407, 49407],\n",
      "        ...,\n",
      "        [49406,   320,  1888,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  2913,  ..., 49407, 49407, 49407],\n",
      "        [49406,   518,  4485,  ..., 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'return_loss': True}\n",
      "{'pixel_values': tensor([[[[0.4549, 0.4549, 0.4667,  ..., 0.2431, 0.2000, 0.2157],\n",
      "          [0.4549, 0.4627, 0.4745,  ..., 0.2275, 0.2353, 0.2471],\n",
      "          [0.4549, 0.4667, 0.4824,  ..., 0.2431, 0.2549, 0.2588],\n",
      "          ...,\n",
      "          [0.3412, 0.3020, 0.3059,  ..., 0.2157, 0.2157, 0.2118],\n",
      "          [0.3098, 0.3490, 0.3059,  ..., 0.2157, 0.2157, 0.2157],\n",
      "          [0.3059, 0.3216, 0.3412,  ..., 0.2157, 0.2196, 0.2118]],\n",
      "\n",
      "         [[0.4196, 0.4275, 0.4353,  ..., 0.2157, 0.1804, 0.1843],\n",
      "          [0.4118, 0.4235, 0.4314,  ..., 0.1961, 0.1922, 0.1922],\n",
      "          [0.4196, 0.4235, 0.4314,  ..., 0.2039, 0.2039, 0.1961],\n",
      "          ...,\n",
      "          [0.1882, 0.1647, 0.1569,  ..., 0.1294, 0.1255, 0.1176],\n",
      "          [0.1725, 0.2000, 0.1647,  ..., 0.1294, 0.1294, 0.1255],\n",
      "          [0.1804, 0.1922, 0.1843,  ..., 0.1294, 0.1333, 0.1255]],\n",
      "\n",
      "         [[0.3882, 0.3961, 0.4235,  ..., 0.1725, 0.1608, 0.1608],\n",
      "          [0.4039, 0.4118, 0.4235,  ..., 0.1765, 0.1804, 0.1686],\n",
      "          [0.4039, 0.4039, 0.4196,  ..., 0.1961, 0.1882, 0.1804],\n",
      "          ...,\n",
      "          [0.1373, 0.1098, 0.1020,  ..., 0.0863, 0.0863, 0.0863],\n",
      "          [0.1176, 0.1412, 0.1020,  ..., 0.0784, 0.0863, 0.0824],\n",
      "          [0.1176, 0.1216, 0.1176,  ..., 0.0784, 0.0824, 0.0824]]],\n",
      "\n",
      "\n",
      "        [[[0.5020, 0.4431, 0.3216,  ..., 0.6980, 0.6353, 0.5765],\n",
      "          [0.3294, 0.1804, 0.0549,  ..., 0.6627, 0.6196, 0.5686],\n",
      "          [0.0745, 0.1294, 0.3843,  ..., 0.6235, 0.5961, 0.5529],\n",
      "          ...,\n",
      "          [0.6235, 0.5922, 0.6627,  ..., 0.5490, 0.5765, 0.5686],\n",
      "          [0.6627, 0.6196, 0.6588,  ..., 0.5216, 0.5569, 0.5843],\n",
      "          [0.6863, 0.6667, 0.6510,  ..., 0.4980, 0.5333, 0.5608]],\n",
      "\n",
      "         [[0.5294, 0.5059, 0.4078,  ..., 0.7373, 0.6784, 0.6196],\n",
      "          [0.3922, 0.2627, 0.1176,  ..., 0.7020, 0.6549, 0.6039],\n",
      "          [0.1255, 0.1647, 0.4118,  ..., 0.6667, 0.6353, 0.5882],\n",
      "          ...,\n",
      "          [0.4667, 0.4471, 0.4941,  ..., 0.4980, 0.5294, 0.5098],\n",
      "          [0.5098, 0.4706, 0.4941,  ..., 0.4706, 0.5137, 0.5216],\n",
      "          [0.5255, 0.5216, 0.4902,  ..., 0.4392, 0.4824, 0.4980]],\n",
      "\n",
      "         [[0.5451, 0.5569, 0.4902,  ..., 0.7059, 0.6549, 0.6039],\n",
      "          [0.4902, 0.3373, 0.1490,  ..., 0.6784, 0.6431, 0.6078],\n",
      "          [0.1647, 0.1686, 0.3922,  ..., 0.6471, 0.6275, 0.5961],\n",
      "          ...,\n",
      "          [0.3804, 0.3765, 0.3725,  ..., 0.4196, 0.4471, 0.4275],\n",
      "          [0.4196, 0.3961, 0.3725,  ..., 0.3843, 0.4235, 0.4275],\n",
      "          [0.4275, 0.4314, 0.3843,  ..., 0.3529, 0.3843, 0.3922]]],\n",
      "\n",
      "\n",
      "        [[[0.5765, 0.5255, 0.8078,  ..., 0.6510, 0.6431, 0.6392],\n",
      "          [0.9137, 0.8157, 0.4275,  ..., 0.6667, 0.6510, 0.6392],\n",
      "          [0.7451, 0.6745, 0.5059,  ..., 0.6627, 0.6471, 0.6510],\n",
      "          ...,\n",
      "          [0.8235, 0.8118, 0.8078,  ..., 0.2667, 0.2314, 0.2471],\n",
      "          [0.8863, 0.8824, 0.8863,  ..., 0.2667, 0.2118, 0.2392],\n",
      "          [0.8902, 0.8902, 0.9020,  ..., 0.2588, 0.2588, 0.2392]],\n",
      "\n",
      "         [[0.5255, 0.4941, 0.7647,  ..., 0.2078, 0.2039, 0.2039],\n",
      "          [0.9098, 0.7765, 0.3490,  ..., 0.2157, 0.2118, 0.2039],\n",
      "          [0.7176, 0.6078, 0.4863,  ..., 0.2118, 0.2039, 0.2000],\n",
      "          ...,\n",
      "          [0.3490, 0.3412, 0.3294,  ..., 0.0667, 0.0745, 0.0667],\n",
      "          [0.3608, 0.3608, 0.3647,  ..., 0.0667, 0.0784, 0.0706],\n",
      "          [0.3569, 0.3647, 0.3804,  ..., 0.0784, 0.0745, 0.0706]],\n",
      "\n",
      "         [[0.7765, 0.5608, 0.8196,  ..., 0.1686, 0.1569, 0.1569],\n",
      "          [1.0000, 0.7098, 0.3686,  ..., 0.1765, 0.1686, 0.1608],\n",
      "          [0.6941, 0.5843, 0.7176,  ..., 0.1725, 0.1608, 0.1725],\n",
      "          ...,\n",
      "          [0.3059, 0.3059, 0.2980,  ..., 0.0431, 0.0431, 0.0431],\n",
      "          [0.3255, 0.3216, 0.3255,  ..., 0.0353, 0.0314, 0.0392],\n",
      "          [0.3176, 0.3137, 0.3255,  ..., 0.0392, 0.0431, 0.0353]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.4588, 0.4549, 0.4431,  ..., 0.8824, 0.8902, 0.8745],\n",
      "          [0.4588, 0.4431, 0.4431,  ..., 0.8902, 0.8784, 0.8392],\n",
      "          [0.4392, 0.4314, 0.4235,  ..., 0.8863, 0.8431, 0.8157],\n",
      "          ...,\n",
      "          [0.5569, 0.5569, 0.5529,  ..., 0.2941, 0.2980, 0.2980],\n",
      "          [0.5725, 0.5765, 0.5608,  ..., 0.2902, 0.3020, 0.3020],\n",
      "          [0.5765, 0.5725, 0.5451,  ..., 0.2863, 0.2902, 0.2980]],\n",
      "\n",
      "         [[0.5255, 0.5216, 0.5176,  ..., 0.9020, 0.9137, 0.9059],\n",
      "          [0.5373, 0.5176, 0.5216,  ..., 0.9098, 0.9020, 0.8706],\n",
      "          [0.5176, 0.5176, 0.5098,  ..., 0.9059, 0.8706, 0.8471],\n",
      "          ...,\n",
      "          [0.5882, 0.5882, 0.5843,  ..., 0.1804, 0.1765, 0.1765],\n",
      "          [0.5961, 0.5961, 0.5765,  ..., 0.1765, 0.1804, 0.1804],\n",
      "          [0.6118, 0.6000, 0.5608,  ..., 0.1725, 0.1686, 0.1765]],\n",
      "\n",
      "         [[0.5569, 0.5569, 0.5529,  ..., 0.9137, 0.9255, 0.9137],\n",
      "          [0.5647, 0.5490, 0.5529,  ..., 0.9216, 0.9137, 0.8784],\n",
      "          [0.5490, 0.5529, 0.5529,  ..., 0.9176, 0.8784, 0.8549],\n",
      "          ...,\n",
      "          [0.6039, 0.6000, 0.5961,  ..., 0.2000, 0.2039, 0.2000],\n",
      "          [0.6235, 0.6196, 0.6000,  ..., 0.1961, 0.2000, 0.2000],\n",
      "          [0.6235, 0.6118, 0.5765,  ..., 0.1922, 0.1922, 0.2000]]],\n",
      "\n",
      "\n",
      "        [[[0.0196, 0.0157, 0.0118,  ..., 0.0000, 0.0000, 0.0078],\n",
      "          [0.0275, 0.0196, 0.0118,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0314, 0.0235, 0.0039,  ..., 0.0235, 0.0196, 0.0078],\n",
      "          ...,\n",
      "          [0.0353, 0.0353, 0.0353,  ..., 0.1529, 0.1216, 0.0863],\n",
      "          [0.0353, 0.0353, 0.0314,  ..., 0.1529, 0.1647, 0.1608],\n",
      "          [0.0471, 0.0353, 0.0235,  ..., 0.1608, 0.1490, 0.1333]],\n",
      "\n",
      "         [[0.0039, 0.0118, 0.0118,  ..., 0.0000, 0.0000, 0.0039],\n",
      "          [0.0078, 0.0118, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0078, 0.0078, 0.0000,  ..., 0.0118, 0.0118, 0.0078],\n",
      "          ...,\n",
      "          [0.0118, 0.0118, 0.0118,  ..., 0.1451, 0.1176, 0.0824],\n",
      "          [0.0118, 0.0118, 0.0118,  ..., 0.1333, 0.1451, 0.1451],\n",
      "          [0.0157, 0.0118, 0.0157,  ..., 0.1412, 0.1333, 0.1294]],\n",
      "\n",
      "         [[0.0000, 0.0039, 0.0078,  ..., 0.0078, 0.0039, 0.0078],\n",
      "          [0.0039, 0.0078, 0.0078,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0078, 0.0039, 0.0000,  ..., 0.0039, 0.0078, 0.0039],\n",
      "          ...,\n",
      "          [0.0118, 0.0118, 0.0118,  ..., 0.1412, 0.1137, 0.0745],\n",
      "          [0.0078, 0.0078, 0.0118,  ..., 0.1294, 0.1412, 0.1412],\n",
      "          [0.0078, 0.0078, 0.0118,  ..., 0.1412, 0.1294, 0.1216]]],\n",
      "\n",
      "\n",
      "        [[[0.3176, 0.3098, 0.3020,  ..., 0.2902, 0.2902, 0.2863],\n",
      "          [0.3176, 0.3098, 0.3098,  ..., 0.2902, 0.2902, 0.2863],\n",
      "          [0.3216, 0.3216, 0.3137,  ..., 0.2902, 0.2941, 0.2902],\n",
      "          ...,\n",
      "          [0.6902, 0.6902, 0.6824,  ..., 0.7333, 0.6627, 0.7412],\n",
      "          [0.7020, 0.6784, 0.6941,  ..., 0.7216, 0.7765, 0.7804],\n",
      "          [0.6784, 0.6863, 0.6863,  ..., 0.7647, 0.8431, 0.8118]],\n",
      "\n",
      "         [[0.6353, 0.6353, 0.6353,  ..., 0.6078, 0.6118, 0.6118],\n",
      "          [0.6353, 0.6314, 0.6353,  ..., 0.6118, 0.6078, 0.6157],\n",
      "          [0.6353, 0.6353, 0.6353,  ..., 0.6157, 0.6118, 0.6118],\n",
      "          ...,\n",
      "          [0.5608, 0.5608, 0.5608,  ..., 0.5882, 0.5098, 0.5725],\n",
      "          [0.5647, 0.5490, 0.5569,  ..., 0.5412, 0.6118, 0.5961],\n",
      "          [0.5451, 0.5529, 0.5490,  ..., 0.5647, 0.6824, 0.6392]],\n",
      "\n",
      "         [[0.8745, 0.8706, 0.8667,  ..., 0.8588, 0.8588, 0.8588],\n",
      "          [0.8745, 0.8667, 0.8667,  ..., 0.8588, 0.8549, 0.8588],\n",
      "          [0.8667, 0.8706, 0.8667,  ..., 0.8588, 0.8588, 0.8588],\n",
      "          ...,\n",
      "          [0.4392, 0.4510, 0.4510,  ..., 0.4471, 0.4196, 0.4667],\n",
      "          [0.4549, 0.4314, 0.4549,  ..., 0.4353, 0.5137, 0.4784],\n",
      "          [0.4431, 0.4392, 0.4431,  ..., 0.4588, 0.5725, 0.4980]]]]), 'input_ids': tensor([[49406,   637,  2368,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  2368,  ..., 49407, 49407, 49407],\n",
      "        [49406,   320,  2368,  ..., 49407, 49407, 49407],\n",
      "        ...,\n",
      "        [49406,   320,  1611,  ..., 49407, 49407, 49407],\n",
      "        [49406,  2176, 10485,  ..., 49407, 49407, 49407],\n",
      "        [49406, 12781,  1500,  ..., 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'return_loss': True}\n"
     ]
    }
   ],
   "source": [
    "for idx,batch in enumerate(train_dataloader):\n",
    "    if idx>4:\n",
    "        print(batch)\n",
    "    if(idx>7):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ceee9c-5683-4a33-b387-5d5c369d34b7",
   "metadata": {},
   "source": [
    "### do_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fec716e7-cc64-49e2-a274-51c61ffe6ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:18 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-df5777364e1d24b5.arrow\n",
      "08/02/2023 14:28:20 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /remote-home/songtianwei/.cache/huggingface/datasets/ydshieh___coco_dataset_script/2017-6f3462d26b47c55a/0.0.0/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f/cache-6125ab8beee00b01.arrow\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if training_args.do_eval:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"validation\" not in dataset:\n",
    "            raise ValueError(\"--do_eval requires a train validation\")\n",
    "        eval_dataset = dataset[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        eval_dataset = eval_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        eval_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0de79590-485c-4880-984a-c131cb1093d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluation dataloader\n",
    "eval_dataloader = torch.utils.data.DataLoader(\n",
    "    eval_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=training_args.eval_batch_size,\n",
    "    num_workers=training_args.dataloader_num_workers,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71221094-a8f5-42ba-a4fc-7b30e8159769",
   "metadata": {},
   "source": [
    "### do_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a79ce7d-26e7-4d07-b80a-e87f010893be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_args.do_predict:\n",
    "    with accelerator.main_process_first():\n",
    "        if \"test\" not in dataset:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        test_dataset = dataset[\"test\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            max_eval_samples = min(len(test_dataset), data_args.max_eval_samples)\n",
    "            test_dataset = test_dataset.select(range(max_eval_samples))\n",
    "    \n",
    "        test_dataset = test_dataset.filter(\n",
    "            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n",
    "        )\n",
    "        test_dataset = test_dataset.map(\n",
    "            function=tokenize_captions,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=[col for col in column_names if col != image_column],\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on test dataset\",\n",
    "        )\n",
    "    \n",
    "        # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "        test_dataset.set_transform(transform_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c12de7-0e67-4d0c-b31d-4c7da0aa8e85",
   "metadata": {},
   "source": [
    "### Normailze fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "960c175b-86d2-4911-af5a-9a5014bf9eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fn(x, mean, std):\n",
    "    return transforms.Normalize(mean=mean, std=std)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3e7b3-290b-4922-bd5d-a57ca42653da",
   "metadata": {},
   "source": [
    "## 9.Initialize the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25082c8f-a4b6-49ba-8fee-95bdd96b935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer\n",
    "use_8bit_adam = False\n",
    "if use_8bit_adam:\n",
    "    try:\n",
    "        import bitsandbytes as bnb\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            \"Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`\"\n",
    "        )\n",
    "\n",
    "    optimizer_cls = bnb.optim.AdamW8bit\n",
    "else:\n",
    "    optimizer_cls = torch.optim.AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8358e129-4555-4631-bc97-963082540582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adamw.AdamW"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "63efdce7-4e75-4094-9aac-ec9596ba3a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_parameters = get_parameter_names(clip_model, ALL_LAYERNORM_LAYERS)\n",
    "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
    "optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.1,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p for n, p in clip_model.named_parameters() if (n not in decay_parameters and p.requires_grad)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "optimizer = optimizer_cls(optimizer_grouped_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1472ae2-0853-474c-a0a6-59384da85567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_model.embeddings.token_embedding.weight',\n",
       " 'text_model.embeddings.position_embedding.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'text_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'text_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'text_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'vision_model.embeddings.patch_embedding.weight',\n",
       " 'vision_model.embeddings.position_embedding.weight',\n",
       " 'vision_model.embeddings.class_embedding',\n",
       " 'vision_model.encoder.layers.0.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.0.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.0.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.0.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.1.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.1.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.1.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.2.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.2.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.2.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.3.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.3.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.3.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.4.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.4.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.4.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.5.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.5.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.5.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.6.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.6.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.6.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.7.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.7.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.7.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.8.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.8.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.8.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.9.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.9.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.9.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.10.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.10.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.10.mlp.fc2.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.k_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.v_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.q_proj.weight',\n",
       " 'vision_model.encoder.layers.11.self_attn.out_proj.weight',\n",
       " 'vision_model.encoder.layers.11.mlp.fc1.weight',\n",
       " 'vision_model.encoder.layers.11.mlp.fc2.weight',\n",
       " 'visual_projection.weight',\n",
       " 'text_projection.weight',\n",
       " 'logit_scale']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf56cf06-957f-4ebc-9e18-1db5b19c0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = 'linear'\n",
    "lr_warmup_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2f0cf978-d9c5-4984-99f9-023fb0420263",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "        lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=lr_warmup_steps * training_args.gradient_accumulation_steps,\n",
    "        num_training_steps=training_args.max_steps * training_args.gradient_accumulation_steps,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e112dc4f-0960-4793-8822-246050ba7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo Optimizer for generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3b172-b841-4a06-8068-110c50e3d147",
   "metadata": {},
   "source": [
    "## 10.Initial About the accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0cdfd78-0bd1-45b0-9d3d-cc19a8e58513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the repository creation\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cfb4d7f3-7455-411e-b78e-4532a34f1686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./clip-roberta-finetuned'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77112bfd-b1d4-4daf-8e6e-8334386dc011",
   "metadata": {},
   "source": [
    "### load model and optimizer and dataloader to accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd8c8ae4-634c-44a8-a3fe-88ea1fc01a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = accelerator.prepare(optimizer)\n",
    "lr_scheduler = accelerator.prepare(lr_scheduler)\n",
    "generator = accelerator.prepare(generator)\n",
    "clip_model = accelerator.prepare(clip_model)\n",
    "    \n",
    "train_dataloader = accelerator.prepare(train_dataloader)\n",
    "eval_dataloader = accelerator.prepare(eval_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a7a513a2-8986-4c55-ad0f-6e7df4d9fd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.to(accelerator.device, dtype=weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34bad-acca-4227-8d72-fd702340ce15",
   "metadata": {},
   "source": [
    "## 11.Initialize max_train_step and tracker ( wandb start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8e75bed4-71a1-4392-8f5f-153f464a91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / training_args.gradient_accumulation_steps)\n",
    "if training_args.max_steps is None or training_args.max_steps <= 0:\n",
    "    training_args.max_steps = training_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "training_args.max_steps = (int)(training_args.max_steps)\n",
    "training_args.num_train_epochs = (int)(training_args.num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dca957db-1862-4ec5-96bc-caed0b3f4fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18a367ca-70e1-41f4-a813-c6dbf7bac805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_update_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fe3c937-4085-4d32-8693-9139c025413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker_project_name = \"text2image-fine-tune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3258147f-8ba0-440b-8239-171a71215133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:22 - ERROR - wandb.jupyter - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2997155472song\u001b[0m (\u001b[33mawyl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2502a00c4ad544ada0649bd6a8a1bb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669420432299374, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/remote-home/songtianwei/research/diffusion_model_my/clip_train/wandb/run-20230802_142829-rh9voh7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q' target=\"_blank\">flowing-forest-183</a></strong> to <a href='https://wandb.ai/awyl/text2image-fine-tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/awyl/text2image-fine-tune' target=\"_blank\">https://wandb.ai/awyl/text2image-fine-tune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q' target=\"_blank\">https://wandb.ai/awyl/text2image-fine-tune/runs/rh9voh7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "# The trackers initializes automatically on the main process.\n",
    "if accelerator.is_main_process:\n",
    "    tracker_config = dict(vars(args))\n",
    "    # tracker_config.pop(\"validation_prompts\")\n",
    "    accelerator.init_trackers(tracker_project_name, tracker_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed935705-fb7f-4778-9dc9-29f0b286da4b",
   "metadata": {},
   "source": [
    "## 12. Start Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff753a-1391-45e4-a4ac-73813b3ddac7",
   "metadata": {},
   "source": [
    "### log training info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4122351f-fd79-4114-ab77-a6fb0925b2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/02/2023 14:28:42 - INFO - __main__ - ***** Running training *****\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Training num examples = 8212\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Evaluation num examples = 25014\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Num Epochs = 3\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Instantaneous batch size per device = 64\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "08/02/2023 14:28:42 - INFO - __main__ -   Total optimization steps = 384\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = training_args.train_batch_size * accelerator.num_processes * training_args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "if args.do_train:\n",
    "    logger.info(f\"  Training num examples = {len(train_dataset)}\")\n",
    "if args.do_eval:\n",
    "    logger.info(f\"  Evaluation num examples = {len(eval_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {training_args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {training_args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {training_args.max_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e8fc25-ec5f-4a3e-8b74-dbc307d98096",
   "metadata": {},
   "source": [
    "### resume from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d487bd3-0d87-4ac2-b285-a1a2256ba0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially load in the weights and states from a previous save\n",
    "if training_args.resume_from_checkpoint:\n",
    "    if training_args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(training_args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "        resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66e4fd10-d54c-474b-8926-589b34a19dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|                                                                                                               | 0/384 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(global_step, training_args.max_steps), disable=not accelerator.is_local_main_process)\n",
    "progress_bar.set_description(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02cb61ef-66bb-4f40-91da-336f15a87fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator.free_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78a164-b92e-41d1-9501-e304e19b9a26",
   "metadata": {},
   "source": [
    "### Training Circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "180ec96c-ac22-46d6-a778-215b0d517a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=1e-4)\n",
    "optimizer = accelerator.prepare(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f6a86579-0c33-4163-a99d-44b0352807e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.num_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214588f7-f75c-44d9-9a49-4072a69f2e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Steps:   0%|                                                                                                               | 0/384 [00:00<?, ?it/s]/remote-home/songtianwei/conda/envs/pytorch2/lib/python3.8/site-packages/torch/nn/modules/module.py:1501: UserWarning: operator() profile_node %580 : int = prim::profile_ivalue(%dtype)\n",
      " does not have profile information (Triggered internally at ../third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  return forward_call(*args, **kwargs)\n",
      "Training Steps:   0%|                                                 | 0/384 [11:58<?, ?it/s, epoch=0, global_step=0, lr=0.0001, step=102, train_loss=4.12]"
     ]
    }
   ],
   "source": [
    "for epoch in range(first_epoch, training_args.num_train_epochs):\n",
    "    if training_args.do_train:\n",
    "        # logging.info(\"*\"*50)\n",
    "        # logging.info(\"Doing Training\")\n",
    "        # logging.info(\"*\"*50)\n",
    "        # if generator_train:\n",
    "        #     generator.train()\n",
    "        # else:\n",
    "        #     generator.eval()\n",
    "            \n",
    "        # if clip_train:\n",
    "        #     clip_model.train()\n",
    "        # else:\n",
    "        #     clip_model.eval()\n",
    "            \n",
    "        progress_bar.set_description(\"Training Steps\")\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # generator_step_M = 1\n",
    "        # clip_step_N = 1\n",
    "        # train_target_list = [\"generator\"]*generator_step_M + [\"clip\"]*clip_step_N\n",
    "        # cur_index = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clip_model.train()\n",
    "            # Skip steps until we reach the resumed step\n",
    "            # if training_args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "            #     if step % training_args.gradient_accumulation_steps == 0:\n",
    "            #         progress_bar.update(1)\n",
    "            #     continue\n",
    "            # which to train\n",
    "            # train_target = train_target_list[cur_index]\n",
    "            # cur_index = (cur_index + 1) % len(train_target_list)\n",
    "            # if train_target == \"generator\":\n",
    "            #     pass\n",
    "\n",
    "            # Convert images to latent space\n",
    "            # img_pixel_values = batch[\"pixel_values\"]  # [6,3,224,224]\n",
    "            # # Get the text embedding for conditioning\n",
    "            # batch_token_ids = batch[\"input_ids\"]\n",
    "            \n",
    "            # generator.zero_grad()\n",
    "            \n",
    "\n",
    "            # generator_train = False\n",
    "            # if generator_train:\n",
    "            #     encoder_hidden_states = text_encoder(batch_token_ids)[0]  # [6,77,768]                \n",
    "            #     noise = generator(img_pixel_values, encoder_hidden_states)\n",
    "                \n",
    "            #     # limit the norm of the noise\n",
    "            #     norm_type = 'l2'\n",
    "            #     epsilon = 16\n",
    "            #     if norm_type == 'l2':\n",
    "            #         temp = torch.norm(noise.view(noise.shape[0], -1), dim=1).view(-1, 1, 1, 1)\n",
    "            #         noise = noise * epsilon / temp\n",
    "            #     else:\n",
    "            #         noise = torch.clamp(noise, -epsilon / 255, epsilon / 255)\n",
    "                    \n",
    "            #     add_noise = False\n",
    "            #     if add_noise:\n",
    "            #         image = img_pixel_values + noise\n",
    "            #     else:\n",
    "            #         image = img_pixel_values + noise * torch.tensor(0.0).to(noise.device)\n",
    "            # else:\n",
    "            #     image = img_pixel_values \n",
    "            # image = img_pixel_values \n",
    "            # image = torch.clamp(image, -1, 1)\n",
    "            \n",
    "            # use_normailize = False\n",
    "            # if use_normailize:\n",
    "            #     image = normalize_fn(image)\n",
    "                \n",
    "            # data_input = {\n",
    "            #     \"input_ids\":batch_token_ids,\n",
    "            #     \"pixel_values\" : image,\n",
    "            #     \"attention_mask\":batch[\"attention_mask\"],\n",
    "            #     \"return_loss\": True\n",
    "            # }\n",
    "            output = clip_model(**batch)\n",
    "            # logits_per_image = output.logits_per_image   # for training , image_logits is the same as logits text\n",
    "            # logits_per_text = output.logits_per_text\n",
    "            \n",
    "            loss = output.loss\n",
    "            \n",
    "            # Gather the losses across all processes for logging (if we use distributed training).\n",
    "            # avg_loss = accelerator.gather(loss.repeat(training_args.train_batch_size)).mean()\n",
    "            # train_loss += avg_loss.item() / training_args.gradient_accumulation_steps\n",
    "\n",
    "            # Backpropagate\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            # if accelerator.sync_gradients:\n",
    "            #     if generator_train:\n",
    "            #         accelerator.clip_grad_norm_(generator.parameters(), training_args.max_grad_norm)\n",
    "            #     elif clip_train:\n",
    "            #         accelerator.clip_grad_norm_(clip_model.parameters(), training_args.max_grad_norm)\n",
    "            \n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "            # lr_scheduler.step()\n",
    "            \n",
    "            clip_model.zero_grad()\n",
    "            # optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            # if accelerator.sync_gradients:\n",
    "            #     progress_bar.update(1)\n",
    "            #     global_step += 1\n",
    "            #     accelerator.log({\"train_loss\": train_loss}, step=global_step)\n",
    "            #     train_loss = 0.0\n",
    "\n",
    "            #     checkpointing_steps = 100\n",
    "            #     if global_step % checkpointing_steps == 0:\n",
    "            #         logging.info(\"Epoch : {} ; Step : {} ; Save checkpoint to {}\".format(epoch, global_step, training_args.output_dir))\n",
    "            #         if accelerator.is_main_process:\n",
    "            #             save_path = os.path.join(training_args.output_dir, f\"checkpoint-{global_step}\")\n",
    "            #             accelerator.save_state(save_path)\n",
    "            #             logger.info(f\"Saved state to {save_path}\")\n",
    "            \n",
    "            record = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": step,\n",
    "                    \"global_step\":global_step,\n",
    "                    \"train_loss\": loss.detach().item(),\n",
    "                    # \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "            wandb.log(record)  \n",
    "            progress_bar.set_postfix(**record)\n",
    "\n",
    "            # if global_step >= training_args.max_steps:\n",
    "            #     break\n",
    "\n",
    "    # evaluation on the eval dataset\n",
    "    # training_args.do_eval = False\n",
    "    # if training_args.do_eval and accelerator.is_main_process:\n",
    "        \n",
    "    #     logging.info(\"*\"*50)\n",
    "    #     logging.info(\"Doing Evaluation\")\n",
    "    #     logging.info(\"*\"*50)\n",
    "    #     progress_bar.set_description(\"Evaluation Steps\")\n",
    "        \n",
    "    #     generator.eval()\n",
    "    #     clip_model.eval()\n",
    "        \n",
    "    #     eval_losses = []\n",
    "    #     for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    #         with torch.no_grad():\n",
    "    #             # Convert images to latent space\n",
    "    #             img_pixel_values = batch[\"pixel_values\"].to(weight_dtype)  # [6,3,224,224]\n",
    "\n",
    "    #             # Get the text embedding for conditioning\n",
    "    #             batch_token_ids = batch[\"input_ids\"]\n",
    "    #             if generator_train:\n",
    "    #                 encoder_hidden_states = text_encoder(batch_token_ids)[0]  # [6,77,768]                \n",
    "    #                 noise = generator.forward(img_pixel_values, encoder_hidden_states)\n",
    "                    \n",
    "    #                 # limit the norm of the noise\n",
    "    #                 norm_type = 'l2'\n",
    "    #                 epsilon = 16\n",
    "    #                 if norm_type == 'l2':\n",
    "    #                     temp = torch.norm(noise.view(noise.shape[0], -1), dim=1).view(-1, 1, 1, 1)\n",
    "    #                     noise = noise * epsilon / temp\n",
    "    #                 else:\n",
    "    #                     noise = torch.clamp(noise, -epsilon / 255, epsilon / 255)\n",
    "                        \n",
    "    #                 add_noise = False\n",
    "    #                 if add_noise:\n",
    "    #                     image = img_pixel_values + noise\n",
    "    #                 else:\n",
    "    #                     image = img_pixel_values + noise * torch.tensor(0.0).to(noise.device)\n",
    "    #             else:\n",
    "    #                 image = img_pixel_values \n",
    "    #             image = torch.clamp(image, -1, 1)\n",
    "                \n",
    "    #             use_normailize = False\n",
    "    #             if use_normailize:\n",
    "    #                 image = normalize_fn(image)\n",
    "                  \n",
    "    #             data_input = {\n",
    "    #                 \"input_ids\":batch_token_ids,\n",
    "    #                 \"pixel_values\" : image\n",
    "    #             }\n",
    "    #             output = clip_model(**data_input, return_loss=True)\n",
    "    #             logits_per_image = output.logits_per_image   # for training , image_logits is the same as logits text\n",
    "    #             logits_per_text = output.logits_per_text\n",
    "                \n",
    "    #             loss = output.loss\n",
    "                \n",
    "    #             # END MY CODE\n",
    "    #         eval_losses.append(loss.detach().item())\n",
    "    #         # logs = {\"step\" : step,  \"lr\": lr_scheduler.get_last_lr()[0],\"eval_loss\": loss.detach().item(),}\n",
    "    #         # progress_bar.set_postfix(**logs)\n",
    "    #     eval_mean_loss = np.mean(eval_losses)\n",
    "    #     eval_record = {\n",
    "    #                 \"epoch\": epoch,\n",
    "    #                 \"global_step\":global_step,\n",
    "    #                 \"eval_avg_loss\": eval_mean_loss,\n",
    "    #                 }\n",
    "    #     wandb.log(eval_record)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81615e-e80a-4330-b928-26ec52ccc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf4e11-7f2d-42d3-b518-067108ac3570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc308ea4-286e-4a2c-bd8c-1e3368c4a183",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023c1486-dd3a-4fff-9b8b-ecc257a205df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe07e3c-03ae-42b0-bc90-3667d7355c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45735-5a70-4790-aebd-40c895e72952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
