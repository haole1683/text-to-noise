{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6b03d9df-0ebd-4c3c-95ce-a766a6833980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from PIL import Image\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    Trainer\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from transformers import CLIPTokenizer, CLIPConfig, CLIPModel\n",
    "from transformers.trainer_pt_utils import get_parameter_names\n",
    "from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\n",
    "\n",
    "import diffusers\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers.utils import is_wandb_available\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.state import AcceleratorState\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "\n",
    "if is_wandb_available():\n",
    "    import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6991471e-8d63-45c3-81ab-52683b260ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_config1 = CLIPConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0a7d83e7-fef5-4954-aba9-a7626d2f13d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.clip.configuration_clip.CLIPConfig"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clip_config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6524e981-4b58-4853-8fc0-1afa68823f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 49406,\n",
       "  \"eos_token_id\": 49407,\n",
       "  \"hidden_act\": \"quick_gelu\",\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 2048,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 77,\n",
       "  \"model_type\": \"clip_text_model\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"projection_dim\": 512,\n",
       "  \"transformers_version\": \"4.31.0\",\n",
       "  \"vocab_size\": 49408\n",
       "}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_config1.text_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "16ef56bb-747e-4940-860c-7750fff76508",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_clip = AutoModel.from_config(clip_config1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66b75c19-fc43-4f01-bc0a-5e3835916541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_config.vision_config.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "22126cdc-6526-4b50-abe3-f163068ff3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(clip_model1.text_model.embeddings,\"token_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eadf57b8-1d80-4b08-b7b5-ced27e0d1dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model1.text_model.embeddings.token_embedding.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df86f4c2-e4a7-4f17-9a87-c59c4054918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"/share/test/songtianwei/model_save\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db59f94b-e813-49b7-85a4-3f75cd8ae311",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model_pre = AutoModel.from_pretrained(\n",
    "    model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e7c889e-dced-448a-ac95-8dc775053c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(clip_model_pre.text_model.embeddings,\"word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9b95a3b-add9-41d5-87ba-fff770007a29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_model_pre.text_model.embeddings.word_embeddings.embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2224c4a-45df-4131-97a7-458001773951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_config.vision_config.num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc1f85-1653-4966-bef5-1e655114eef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352289dc-5a9f-43f5-acc9-19e7a135f232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b1ccb-8b99-443e-976c-cc47fdca59a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626b5d9-696a-4809-baf0-eafc67f50c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4a871-3024-4c3c-a60b-65bb28b30fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2064c-5410-405b-a87b-ba646d8298aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6947c47-12ff-4222-a216-7230932a3d51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80662550-2ae8-4fc8-8274-e49b727e01d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c04be-a041-4499-af9e-7acb57b2c3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662988a4-e254-44f0-b6b1-e07428455009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d216c21-39d1-4eeb-9e08-bc67fd74a681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8252c265-5422-487f-9988-a6ab3e00f829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b059201-696a-4f22-9cc4-9ce104c460be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d94f6-c075-4d8e-af3a-c0aa2e83bc7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52e3e6-8979-49ef-87e2-adb09935d1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac7ba0d-15a6-41e4-b4b9-c399ee18e07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d38b31-8f50-419f-8a8d-d312bab0a346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96cb286b-b377-4166-8f5d-594b720dc317",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=1,\n",
    "        mixed_precision=\"no\",\n",
    "   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ccd947-0d83-419e-8321-893a73fe0574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accelerator.use_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a585602-a4f5-443a-bffe-614762e90eb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "vae_config = {\n",
    "    'sample_size': [224,224],  # 512\n",
    "    'in_channels': 3,\n",
    "    'out_channels': 3,\n",
    "    'down_block_types': ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "    'up_block_types': ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "    'block_out_channels': [128, 256, 512],\n",
    "    'layers_per_block': 2,\n",
    "    'act_fn': 'silu',\n",
    "    'latent_channels': 4,\n",
    "    'norm_num_groups': 32,\n",
    "    'scaling_factor': 0.18215,\n",
    "}\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd3155eb-0308-423c-a5f1-198a010ab3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL(**vae_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eccbd3a6-1be4-428d-9d63-101d598d5634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_config = vae.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7714291-a1d0-4821-b8bd-36d19b3a27da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict([('in_channels', 3),\n",
       "            ('out_channels', 3),\n",
       "            ('down_block_types',\n",
       "             ['DownEncoderBlock2D',\n",
       "              'DownEncoderBlock2D',\n",
       "              'DownEncoderBlock2D']),\n",
       "            ('up_block_types',\n",
       "             ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D']),\n",
       "            ('block_out_channels', [128, 256, 512]),\n",
       "            ('layers_per_block', 2),\n",
       "            ('act_fn', 'silu'),\n",
       "            ('latent_channels', 4),\n",
       "            ('norm_num_groups', 32),\n",
       "            ('sample_size', [224, 224]),\n",
       "            ('scaling_factor', 0.18215),\n",
       "            ('force_upcast', True),\n",
       "            ('_use_default_values', ['force_upcast'])])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2dbb80f-33fb-453b-9eff-7ceebd5d6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((4,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20663c11-913e-4d18-afb3-f9647f6710ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_latent = vae.encode(a).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d182178-d12e-4a29-af4e-dda458af5799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 56, 56])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d242fbae-2536-4b2d-9d59-648ce6bd529a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_config[\"out_channels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13a3e54a-f2cc-4fe5-8475-54aaea4df185",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "unet_config = {\n",
    "    \"in_channels\": vae_config[\"latent_channels\"],\n",
    "    \"out_channels\": vae_config[\"latent_channels\"],\n",
    "    \"sample_size\": 56,\n",
    "    \"act_fn\": \"silu\",\n",
    "    \"attention_head_dim\": 8,\n",
    "    \"block_out_channels\": [\n",
    "        320,\n",
    "        640,\n",
    "        1280,\n",
    "        1280\n",
    "    ],\n",
    "    \"center_input_sample\": False,\n",
    "    \"cross_attention_dim\": 512,  \n",
    "    \"down_block_types\": [\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",\n",
    "        \"DownBlock2D\"\n",
    "    ],\n",
    "    \"downsample_padding\": 1,\n",
    "    \"flip_sin_to_cos\": True,\n",
    "    \"freq_shift\": 0,\n",
    "    \"layers_per_block\": 2,\n",
    "    \"mid_block_scale_factor\": 1,\n",
    "    \"norm_eps\": 1e-05,\n",
    "    \"norm_num_groups\": 32,\n",
    "    \"up_block_types\": [\n",
    "        \"UpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\",\n",
    "        \"CrossAttnUpBlock2D\"\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9def99ad-070a-4d22-a3b6-fe271335b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet2DConditionModel(**unet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ba47cd7-ac98-4bb2-98f6-ecb3814ce6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = torch.ones([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c098292-d440-498e-bf07-8273dc37a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.ones((4,77,768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "adc4e18c-c433-413f-9380-f90325484d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states = torch.ones((4,77,512))  # goes wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "692b3bbe-64af-4bac-a7c7-0baa29c37419",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_latent2 = unet(a_latent,timestep,encoder_hidden_states).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d824ae3-300b-4c89-a9db-2c4df3a0e28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 56, 56])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_latent2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3bd67d2b-09d3-4ffb-a48b-23125d8a0d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = vae.decoder(a_latent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6d154b3e-e6cc-42c7-a0cc-ef0797745829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcca23d-4ec9-487b-a64c-40de0279dd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b83ec7-1581-4c5d-82d5-3e8f4f051d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ef9a1-3721-4e7b-b71c-b320a5044372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11800392-403e-465d-884b-35a6151c6207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49695f-300b-4d95-9ae0-24550e44f896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b570ab-1ae4-451b-9de2-5c1626c34c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee42ec-c936-4511-bd43-f8763302835d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69eec2-d104-4f0c-95f8-c02b524767dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ecbef-bf55-4929-a1f6-c68fbcd56315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ac0a6-235e-45cf-b740-e71e511e5109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ab0d7-9101-4570-895c-440d3d293215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72c395ac-92c7-4cc7-a89b-197ff20df7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "705175f1-75e7-4152-9d26-2d61d398f10c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, image_channel=3, image_shape=[224,224], text_embedding_dim=512):\n",
    "        super(Generator, self).__init__()\n",
    "        self.image_channel = image_channel\n",
    "        self.image_shape = image_shape if isinstance(image_shape, list) else [image_shape, image_shape]\n",
    "        self.text_embedding_dim = text_embedding_dim\n",
    "        \n",
    "        self.vae_config = {\n",
    "            'sample_size': self.image_shape,  # 512\n",
    "            'in_channels': self.image_channel,\n",
    "            'out_channels': self.image_channel,\n",
    "            'down_block_types': ['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "            'up_block_types': ['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "            'block_out_channels': [128, 256, 512, 512],\n",
    "            'layers_per_block': 2,\n",
    "            'act_fn': 'silu',\n",
    "            'latent_channels': 4,\n",
    "            'norm_num_groups': 32,\n",
    "            'scaling_factor': 0.18215,\n",
    "        }\n",
    "         \n",
    "        self.vae = AutoencoderKL(**self.vae_config)\n",
    "        \n",
    "        self.unet_config = {\n",
    "            \"in_channels\": self.vae_config[\"latent_channels\"],\n",
    "            \"out_channels\": self.vae_config[\"latent_channels\"],\n",
    "            \"sample_size\": 28,\n",
    "            \"act_fn\": \"silu\",\n",
    "            \"attention_head_dim\": 8,\n",
    "            \"block_out_channels\": [\n",
    "                320,\n",
    "                640,\n",
    "                1280,\n",
    "                1280\n",
    "            ],\n",
    "            \"center_input_sample\": False,\n",
    "            \"cross_attention_dim\": self.text_embedding_dim,  \n",
    "            \"down_block_types\": [\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"CrossAttnDownBlock2D\",\n",
    "                \"DownBlock2D\"\n",
    "            ],\n",
    "            \"downsample_padding\": 1,\n",
    "            \"flip_sin_to_cos\": True,\n",
    "            \"freq_shift\": 0,\n",
    "            \"layers_per_block\": 2,\n",
    "            \"mid_block_scale_factor\": 1,\n",
    "            \"norm_eps\": 1e-05,\n",
    "            \"norm_num_groups\": 32,\n",
    "            \"up_block_types\": [\n",
    "                \"UpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\",\n",
    "                \"CrossAttnUpBlock2D\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        self.unet = UNet2DConditionModel(**self.unet_config)\n",
    "        \n",
    "        \n",
    "    def forward(self, img_pixel_values, encoder_hidden_states):\n",
    "        latent = self.vae.encode(img_pixel_values).latent_dist.sample()\n",
    "        timesteps = torch.randint(0, 1000, (1,),device=latent.device)\n",
    "        timesteps = timesteps.long()  #  6\n",
    "        unet_pred = self.unet(latent, timesteps, encoder_hidden_states).sample\n",
    "        vae_decoding = self.vae.decoder(unet_pred)\n",
    "        return vae_decoding\n",
    "    \n",
    "    \n",
    "    def enable_xformers_memory_efficient_attention(self):\n",
    "        self.unet.enable_xformers_memory_efficient_attention()\n",
    "        self.vae.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7792e64b-528b-4e85-a9fa-ea3bb929f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_ids = torch.ones((4,77),dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f70eb54f-f1a6-46b1-9b5b-b4783cb071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_attention_mask = torch.ones((4,77),dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa51c78e-8ace-4fc4-8911-6af185c5f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_pre = clip_model_pre.text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b12f78a1-1c88-4930-89c2-3cd08f557733",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d22b7cd5-b84e-437e-94b4-f069369859e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 768])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_pre(batch_input_ids,batch_attention_mask)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cf44c9b3-eb2e-43c5-badb-b241cac6f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_clip = clip_model_clip.text_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a1cb0beb-2df1-46ce-a610-ebbd70fd1759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextTransformer(\n",
       "  (embeddings): CLIPTextEmbeddings(\n",
       "    (token_embedding): Embedding(49408, 512)\n",
       "    (position_embedding): Embedding(77, 512)\n",
       "  )\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): QuickGELUActivation()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d75adaa8-bd86-4b95-b42f-abf98f7fea57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 512])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder_clip(batch_input_ids,batch_attention_mask)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fbe8b-ffaf-487c-9c1e-2177a8853008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
